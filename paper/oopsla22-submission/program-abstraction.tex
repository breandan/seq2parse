\section{Abstracting Programs with Parse Errors}
\label{sec:prog-abstract}

We start by introducing our approach for abstracting programs with parse errors
into a suitable sequence of tokens for training sequence classifiers. We explain
how a traditional Earley parser can be used to extract partial parses with the
aid of a Probabilistic Context-Free Grammar (PCFG), in order to get a higher
level of abstraction with richer context information than the simple
\emph{Lexer} output.

\input{api.tex}

\mypara{Lexical Analysis}
Historically, \emph{lexical analysis}, lexing or tokenization is the process of
converting a sequence of characters \ie a program into a sequence of tokens
(strings with an assigned and thus identified meaning). The program that
performs lexical analysis is called a \emph{lexer} and is usually combined with
a parser, which together analyze the syntax of a programming language $L(G)$,
defined by the grammar $G$. When a program has a syntax error, the output token
sequence of the lexer is the highest level of abstraction that we can acquire,
since the parser fails and returns an error.

\mypara{Token Sequences}
Our goal is to parse a \emph{program token sequence} $t^i$, which is a lexed
program with parse errors (\ie $t^i \notin L(G)$), and repair it into a
\emph{fixed token sequence} $t^o \in L(G)$ that can be used to return a repaired
program without syntax errors. Let $t^i$ be a sequence $t^i_1, t^i_2, \dots,
t^i_n$ and $t^o$ be the updated sequence $t^o_1, t^o_2, \dots, t^o_i, \dots,
t^o_j, \dots, t^o_k$. The subsequence $t^o_i, \dots, t^o_j$ can either
\emph{replace} a subsequence in $t^i$, it can \emph{be inserted} in $t^i$ or can
be the empty subsequence $\epsilon$ and essentially \emph{delete} a subsequence
in $t^i$ to generate the $t^o$. It can be the whole program, part of it or
multiple parts of it. $t^o$ will finally be a token sequence that can be parsed
by the original language's $L(G)$ parser.

However, programs can be large and, thus, $n$ can take large values, which makes
them unsuitable for training effectively sequence models. Therefore, our goal is
to first generate an \emph{abstracted token sequence} $t^a$ that removes all
irrelevant information from $t^i$ and gives hints for the parse error fix by
using the internal states of an \emph{Earley} parser.


\subsection{Earley Partial Parses}
\label{sec:prog-abstract:partial}

We propose using an \emph{Earley parser} to generate the abstracted token
sequence $t^a$ for an input program sequence $t^i$. An Earley parser holds
internally a \emph{chart} data structure, \ie \emph{a list of partial parses}.
Given a production rule $X \rightarrow \alpha \beta$, the notation $X
\rightarrow \alpha \cdot \beta$ represents a condition in which $\alpha$ has
already been parsed and $\beta$ is expected and both are sequences of terminal
and non-terminal symbols (tokens).

Each state is a tuple $(X \rightarrow \alpha \cdot \beta,\ j)$, consisting of
\begin{itemize}
    \item the production rule currently being matched $(X \rightarrow \alpha
    \beta)$
    \item the current position in that production (represented by the dot
    $\cdot$)
    \item the origin position $j$ in the input at which the matching of this
    production began
\end{itemize}

We denote the state set at an input position $k$ as $S(k)$. The parser is seeded
with $S(0)$ consisting of only the top-level rule $S \rightarrow \gamma$. It
then repeatedly executes three operations: \emph{prediction, scanning,} and
\emph{completion}. There exists a \emph{complete parse} if the complete
top-level rule $(S \rightarrow \gamma \cdot, 0)$ is found in $S(n)$, where $n$
the input length. We define a \emph{partial parse} to be any partially completed
rules, \ie if there is $(X \rightarrow \alpha \cdot \beta, i)$ in some state
$S(k)$, where $i < k \leq n$.

Let, again, $t^i_1, t^i_2, \dots, t^i_j, \dots, t^i_k, \dots, t^i_n$ be the
input token sequence $t^i$, where at location $k$ there is a parse error and the
parser has exhausted all possibilities and can not add any more rules in state
$S(k + 1)$. We want to abstract $t^i$ by getting the longest possible part of
the program that has a partial parse, \ie by finding the largest $j$ for which
there is a rule $(X \rightarrow \alpha \cdot \beta, 0) \in S(j)$. We use this
rule for $X$ to replace $t^i_1, t^i_2, \dots, t^i_j$ in $t^i$ with $\alpha$,
thus getting an abstracted sequence $t^a$. In the same manner, we use the
longest possible partial parses that we can extract from the chart to abstract
$t^i_{j+1}, \dots, t^i_k$, until we reach the parse error at location $k$, where
$S(k + 1) = \emptyset$.

% \mypara{Problem: Multiple Partial Parses}
%
However, each of the states $S(j),\ 0 \leq j \leq k$, holds a large number of
partial parses and, thus, our heuristic to choose the longest possible partial
parse to abstract programs may not be able to abstract the token sequence fully
until the error location $k$. Additionally, there may be two or more partial
parses in $S(k)$, with different lengths, \eg $\{(X \rightarrow \alpha \cdot
\beta,\ j),\ (X' \rightarrow \alpha' \cdot \beta',\ h)\} \in S(k),\ j \neq h$.
We propose selecting the most \emph{probable parse} with the aid of a PCFG.


\subsection{Probabilistic Context-Free Grammars}
\label{sec:prog-abstract:pcfg}
We learn a PCFG from a large corpus of programs $\List{e}, e \in L(G)$, that
belong to a language $L(G)$ that a grammar $G$ defines, with $\learnPCFGsym$ as
shown in \autoref{fig:api}. We use the learned PCFG with an augmented Earley
parser in $\partialsym$ to abstract a program $e_{\bot}$ into a abstract token
sequence $t^a$.

% \mypara{Learning a PCFG}
%
A PCFG can be defined similarly to a \emph{context-free grammar} $G \defeq (N,\
\Sigma,\ P,\ S)$ as a quintuple $(N,\ \Sigma,\ P,\ S,\ W)$, where:
\begin{itemize}
    \item $N$ and $\Sigma$ are finite disjoint alphabets of non-terminals and
    terminals, respectively.
    \item $P$ is a finite set of production rules of the form $X \rightarrow
    \alpha$, where $X \in N$ and $\alpha \in (N \cup \Sigma)^{\ast}$.
    \item $S$ is a distinguished start symbol in $N$.
    \item $W$ is a finite set of probabilities $p(X \rightarrow \alpha)$ on
    production rules.
\end{itemize}

Given a dataset of programs $\List{e}, e \in L(G)$ that can be parsed, let
$count(X \rightarrow \alpha)$ be the number of times the production rule $X
\rightarrow \alpha$ has been used to generate a final complete parse, while
parsing $\List{e}$, and $count(X)$ be the number of times the non-terminal $X$
has been seen in left side of a used production rule. The probability for a
production rule $X \rightarrow \alpha$ is then defined as:

\begin{equation*}
    p(X \rightarrow \alpha) = \frac{count(X \rightarrow \alpha)}{count(X)}
\end{equation*}

% Of course, under this definition we have the constraint that for any $X \in N$:

% \begin{equation*}
%     \sum_{X \rightarrow \alpha \in P, \alpha \in (N \cup \Sigma)^{\ast}}{p(X \rightarrow \alpha)} = 1
% \end{equation*}

$\learnPCFGsym$ invokes an instrumented Earley parser to calculate all the
values $count(X \rightarrow \alpha), \forall X \rightarrow \alpha: P$ and
$count(X), \forall X: N$. The instrumented parser keeps a \emph{global record}
of these values, while parsing the dataset $\List{e}$ of programs. Finally,
$\learnPCFGsym$ outputs a PCFG that is based on the original grammar $G$ that
was used to parse the dataset with the learned probabilities $W$.

\subsection{Abstracted Token Sequences}

Given a program $e_{\bot}$ with a parse error and a learned PCFG, $\partialsym$
will generate an abstracted token sequence $t^a$. The PCFG will be used with an
augmented Earley parser to disambiguate partial parses and choose one, in order
to produce an abstracted token sequence as described in
\S~\ref{sec:prog-abstract:partial}.

% \mypara{Probabilistic Earley Parsing}
%
We augment Earley states $(X \rightarrow \alpha \cdot \beta,\ j)$ to $(X
\rightarrow \alpha \cdot \beta,\ j,\ p)$, where $p$ is the probability that $X
\rightarrow \alpha \cdot \beta$ is a correct partial parse. When there are two
(or more) conflicting partial parses $\{(X \rightarrow \alpha \cdot \beta,\ j,\
p),\ (X' \rightarrow \alpha' \cdot \beta',\ h,\ p')\} \in S(k)$, the augmented
parser selects the partial parse with the highest probability $max(p,\ p')$. The
augmented parser calculates the probability $p$ for a partial parse $(X
\rightarrow \alpha \cdot \beta,\ j,\ p)$ in the state $S(k)$, as the product
$p_1 \cdot p_2 \cdot \dots \cdot p_{k-1}$ of the probabilities $p_1,\ p_2,\
\dots,\ p_{k-1}$ that are associated with the production rules $(X_1 \rightarrow
\alpha_1 \cdot \beta_1,\ i_1,\ p_1), (X_2 \rightarrow \alpha_2 \cdot \beta_2,\
i_2,\ p_2), \dots$ that have been used so far to parse the string of tokens
$\alpha$.
