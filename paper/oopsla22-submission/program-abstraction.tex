\section{Abstracting Programs with Parse Errors}
\label{sec:prog-abstract}

We start by introducing our approach for abstracting programs with parse errors
into a suitable sequence of tokens for training sequence classifiers. We explain
how a traditional Earley parser can be used to extract partial parses, along
with a Probabilistic Context-Free Grammar (PCFG), in order to get a better level
of abstraction with richer context information than the simple \emph{Lexer}
output.

\input{api.tex}

\mypara{Lexical Analysis}
\emph{Lexical analysis}, lexing or tokenization is the process of converting a
sequence of characters \ie a program into a sequence of tokens (strings with an
assigned and thus identified meaning). The program that performs lexical
analysis is called a \emph{lexer} and is usually combined with a parser, which
together analyze the syntax of a programming language $L$. However, when a
program has a syntax error, the output token sequence of the lexer is the only
level of abstraction that we can acquire for such a program, since the parser
returns an error.

\mypara{Token Sequences}
Our goal is to repair a \emph{program token sequence} $t_i$, which is a lexed
program with parse errors (\ie $t_i \notin L$), into a \emph{fixed token
sequence} $t_o \in L$ that can be used to return a repaired program without
syntax errors. Let $t_i$ be a sequence $T_1, T_2, \dots, T_n$ and $t_o$ be the
updated sequence $T_1, T_2, \dots, T_i, \dots, T_j, \dots, T_k$. The subsequence
$T_i, \dots, T_j$ is a part of $t_i$ that has been replaced, deleted or inserted
in order to generate the $t_o$. It can be the whole program, part of it or
multiple parts of it. The $t_o$ will finally be a token sequence that can be
parsed by the original language's $L$ parser.

However, programs can be large, \ie $n$ can take a really large value, which
makes it unsuitable for training effectively sequence models. These large
programs can also contain a lot of irrelevant information for fixing a specific
parse error, \eg if in our running example in \autoref{fig:orig-prog} there was
another correct function definition before the one with the parse error.
Therefore, our goal is to first generate a \emph{abstracted token sequence}
$t_a$ that removes all irrelevant information from $t_i$ and gives hints for the
parse error fix by using the internal states of an \emph{Earley} parser.


\subsection{Earley Partial Parses}
\label{sec:prog-abstract:partial}

We propose using an \emph{Earley parser} to generate the abstracted token
sequence $t_a$ for an input program sequence $t_i$. An Earley parser holds
internally a \emph{chart} data structure, \ie \emph{a list of partial parses}.
Given a production rule $X \rightarrow \alpha \beta$, the notation $X
\rightarrow \alpha \cdot \beta$ represents a condition in which $\alpha$ has
already been parsed and $\beta$ is expected and both are sequences of terminal
and non-terminal symbols.

Each state is a tuple $(X \rightarrow \alpha \cdot \beta,\ i)$, consisting of
\begin{itemize}
    \item the production currently being matched $(X \rightarrow \alpha \beta)$
    \item the current position in that production (represented by the dot
    $\cdot$)
    \item the position $i$ in the input at which the matching of this production
    began: the origin position
\end{itemize}

The state set at input position $k$ is called $S(k)$. The parser is seeded with
$S(0)$ consisting of only the top-level rule. The parser then repeatedly
executes three operations: \emph{prediction, scanning,} and \emph{completion}.
There exists a \emph{complete parse} if $(X \rightarrow \gamma \cdot, 0)$ ends
up in $S(n)$, where $(X \rightarrow \gamma)$ is the top level-rule and $n$ the
input length. We define as a \emph{partial parse} any partially completed rule,
\ie if there is $(X \rightarrow \alpha \cdot \beta, i)$ in some state $S(k)$,
where $i < k < n$.

Let, again, $T_1, T_2, \dots, T_i, \dots, T_k, \dots, T_n$ be the an input token
sequence $t_i$, where in location $k$ there is a parse error and the Earley
parser can not add any more rules in state $S(k + 1)$. We abstract $t_i$ by
getting the longest possible part of the program that has a partial parse, \ie
by finding the largest $i$ for which there is a rule $(X \rightarrow \alpha
\cdot \beta, 0) \in S(i)$. We use this rule for $X$ to replace $T_1, T_2, \dots,
T_i$ in $t_i$ with $\alpha$, thus getting an abstracted sequence $t_a$. We
continue abstracting the program until we reach the parse error and have no
states to use, \ie where $S(k + 1) = \emptyset$. In the same manner, we use the
longest possible partial parses that we can extract from the chart to abstract
$T_{i+1}, \dots, T_k$.

\mypara{Problem: Multiple Partial Parses} Each of the states $S(i)$, where $0
\leq i \leq k$, holds a large number of uncompleted rules, \ie partial parses.
We used as a heuristic to choose the longest possible partial parses to abstract
our erroneous programs and thus limiting the size of the program token sequence
$t_i$ as much as possible. However, it is possible that we have two or more such
parses in $S(k)$, where they have the same length, \eg $\{(X \rightarrow \alpha
\cdot \beta,\ i),\ (X' \rightarrow \alpha' \cdot \beta',\ i)\} \in S(k)$.


\mypara{Example}


\subsection{Probabilistic Context-Free Grammars}
\label{sec:prog-abstract:pcfg}
We propose learning a \emph{Probabilistic Context-Free Grammar} (PCFG),
$\learnPCFGsym$, from a large corpus of programs $\List{e}, e \in L(G)$, that
belong to a language $L(G)$ that a grammar $G$ defines, as shown in
\autoref{fig:api}. We use the learned PCFG within an augmented Earley parser in
$\partialsym$ to abstract a program $e_{\bot}$ into a abstract token sequence
$t_a$.

\mypara{Learning a PCFG}
A PCFG can be defined similarly to a \emph{context-free grammar} $G \defeq (N,\
\Sigma,\ P,\ S)$ as a quintuple $(N,\ \Sigma,\ P,\ S,\ W)$, where:
\begin{itemize}
    \item $N$ and $\Sigma$ are finite disjoint alphabets of non-terminals and
    terminals, respectively.
    \item $P$ is a finite set of production rules of the form $X \rightarrow
    \alpha$, where $X \in N$ and $\alpha \in (N \cup \Sigma)^{\ast}$.
    \item $S$ is a distinguished start symbol in $N$.
    \item $W$ is a finite set of probabilities $p(X \rightarrow \alpha)$ on
    production rules.
\end{itemize}

Given a dataset of programs $\List{e}, e \in L(G)$ that can be parsed, let
$count(X \rightarrow \alpha)$ be the number of times the production rule $X
\rightarrow \alpha$ has been used to gernerate a final parse, while parsing
$\List{e}$, and $count(X)$ be the number of times the non-terminal $X$ has been
seen. The probability for a production $X \rightarrow \alpha$ is then defined
as:

\begin{equation*}
    p(X \rightarrow \alpha) = \frac{count(X \rightarrow \alpha)}{count(X)}
\end{equation*}

Of course, under this definition we have the constraint that for any $X \in N$:

\begin{equation*}
    \sum_{X \rightarrow \alpha \in P, \alpha \in (N \cup \Sigma)^{\ast}}{p(X \rightarrow \alpha)} = 1
\end{equation*}

Therefore, $\learnPCFGsym$ invokes an instrumented Earley parser to calculate
all possible counters $count(X \rightarrow \alpha), \forall X \rightarrow
\alpha: P$ and $count(X), \forall X: N$. The instrumented Earley parser keeps a
\emph{global record} for all these counters, while parsing the dataset
$\List{e}$ of programs. Finally, $\learnPCFGsym$ will output a PCFG that is
based on the original grammar $G$ that was used to parse the dataset with the
additional probabilities.

\mypara{Example of PCFG}

\subsection{From Programs to Abstract Token Sequences}

Given a program $e_{\bot}$ with a parse error and a learned PCFG, $\partialsym$
will generate an abstracted token sequence $t_a$. The PCFG will be used with an
augmented Earley parser to disambiguate partial parses and choose one, in order
to produce an abstracted token sequence as described in
\S~\ref{sec:prog-abstract:partial}.

\mypara{Probabilistic Earley Parsing}
We extend the state $(X \rightarrow \alpha \cdot \beta,\ i)$ that the Earley
parser holds within its chart to $(X \rightarrow \alpha \cdot \beta,\ i,\ p)$,
where $p$ is the probability that $X \rightarrow \alpha \cdot \beta$ is a
correct partial parse. Therefore, when the extended Earley parser, when we have
two (or more) conflicting partial parses $\{(X \rightarrow \alpha \cdot \beta,\
i,\ p),\ (X' \rightarrow \alpha' \cdot \beta',\ i,\ p')\} \in S(k)$, we select
the partial parse that the largest probability $max(p,\ p')$. The extended
Earley parser calculates the probability $p$ for a partial parse $(X \rightarrow
\alpha \cdot \beta,\ i,\ p)$ in the state $S(k)$, as the product $p_1 \cdot p_2
\cdot ... \cdot p_{k-1}$ of the probabilities $p_1,\ p_2,\ ...,\ p_{k-1}$ that
are associated with the partial parses that have been used so far to parse the
string of tokens $\alpha$.

\mypara{Example: Selecting Most Likely Partial Parse}



