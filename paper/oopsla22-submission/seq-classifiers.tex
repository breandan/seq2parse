\section{Training Sequence Classifiers}
\label{sec:seq-classifiers}

Our next task is to \emph{train} a model that can predict which error production
rules are needed to parse a given program (with syntax errors) according to a
given grammar $G$ and given its (abstracted) program token sequence $t^a$.
%
We do so by defining a function $\predictDLsym$ which takes as input a
\emph{pre-trained Deep Neural Network (\dnn)} model $\Model$ and an abstracted
token sequence $t^a$.
%
It returns as output a \emph{fixed set of error production rules}
$\errorrulessym$ which can be used along with an error correcting parser
(ECE-Parser) (\autoref{sec:whole-system}) to successfully parse the program.
%
We train the $\Model$ with the $\trainDLsym$ method. $\trainDLsym$ gets a
dataset of token sequences $t^a$ and the \emph{exact small set} of error
production rules that the ECE-Parser needs to generate the intended user parse,
\ie the dataset $\List{t^a \times \errorrulessym}$.

Next, we provide some necessary background about \emph{\dnn{}s} and
\emph{Sequence-to-sequence (seq2seq)} models and describe how we incorporate
them in the problem of selecting a small set of error production rules, which we
frame as a instance of \emph{multi-class classification}. We leave the high
level details of acquiring the dataset of labeled token sequences and using the
predictor for new erroneous programs for \autoref{sec:whole-system}.

\mypara{Background: Neural Networks}
A neural network (or feed-forward network) can be represented as a directed
acyclic graph whose nodes are arranged in layers that are fully connected by
weighted edges. The first layer corresponds to the input features, and the final
to the output. The output of an internal node is the sum of the weighted outputs
of the previous layer passed to a non-linear function, called the
\emph{activation function}. For example, the outputs of an internal layer of
nodes $l$ is given as:
\begin{equation*}
    y_l = g(W_{l-1} y_{l-1})
\end{equation*}
where $W_{l-1}$ is the weight matrix for the edges between layers $l$ and $l-1$
and $y_{l-1}$ is the output of the previous layer. The input $y_0 = x$ is the
input features of the neural network and, finally, g is the activation function,
which in recent work is commonly chosen to be the rectified linear unit (ReLU),
defined as $g(x) = max(0, x)$ \citep{Nair2010-xg}.  The number of layers, the
number of nodes per layer, and the connections between layers constitute the
\emph{architecture} of a neural network. In this work, we use relatively
\emph{deep neural networks} (\dnn). A thorough introduction to neural networks
is beyond the scope of this work~\citep{Hastie2009-bn, Nielsen2015-pu}.


\subsection{Sequence Models}
\label{sec:seq-classifiers:seq-models}

\mypara{Background: Sequence-To-Sequence Models}
Sequence-to-sequence (seq2seq) models aim to transform input sequences of one
domain into sequences of another domain \citep{Sutskever_2014}. In the general
case, these models consist of two major layers, an \emph{encoder} and a
\emph{decoder}. The encoder transforms an input token sequence $x_1, x_2, \dots,
x_n$ into a \emph{abstract vector} $V \in \R^k$ that captures all the essence
and context of the input sequence. This vector doesn't necessarily have some
physical meaning and is just an internal representation of the input sequence
into a higher dimensional space. The abstract vector is then given as an input
to the decoder, which in turn transforms it into an output sequence $y_1, y_2,
\dots, y_n$.

\begin{align}
    h_t &= f(W_{hx} x_t + W_{hh} h_{t-1}) \label{eq:1} \\
    y_t &= g(W_{yh} h_t) \label{eq:2}
\end{align}

The simplest approach historically uses a Recurrent Neural Network (RNN)
\citep{Rumelhart1986, Werbos1990, Hochreiter_1997}, which is a natural next step
from the classic neural networks. Each RNN unit however operates on each input
token $x_t$ separately. It keeps an internal \emph{hidden state} $h_t$ that is
calculated as in \autoref{eq:1}, as a function of the input token $x_t$ and the
previous hidden state $h_{t-1}$. The weight matrices $W_{hx}$ and $W_{hh}$
parametrize the input-to-hidden and the hidden-to-hidden connections
respectively. The function $f$ is also a non-linear activation function such as
\emph{tanh, sigmoid} and \emph{ReLU}. The output $y_t$ at position $t$ is
calculated as the product of the current hidden state $h_t$ and the weight
matrix $W_{yh}$ (\autoref{eq:2}). The activation function $g$ is usually chosen
as the standard \emph{softmax} function, which, for an output vector $y = (y_1,
\dots, y_N) \in \R^{N}$, is defined as:
\[ \sigma(y)_i = \frac{e^{y_i}}{\sum_{j=1}^{N} e^{y_j}},\ for\ i = 1, \dots, N.
\] Finally, the loss function at all steps of the RNN is typically calculated
as the sum of the cross-entropy loss of each step.

The recurrent models such as the RNN, have to generate the sequence of hidden
states $h_t$, as a function of the previous hidden state $h_{t-1}$ and the input
for position $t$. This sequential generation of hidden states limits
parallelization within training examples and causes training problems when
dealing with longer sequences, as memory constraints limit batching across
examples. However, the latest state-of-the-art approach for the seq2seq
architecture is the newer \emph{Transformer} model \citep{Vaswani_2017} that
replaces the classic RNN unit in the encoder and decoder architectures and
solves these problems due to its \emph{attention mechanisms}.

\mypara{Transformers}
Attention mechanisms have been of interest lately \citep{Bahdanau2015, Kim2017,
Vaswani_2017} mainly due to their ability to detect dependencies in the input or
output sequences regardless the distance of the tokens. The Transformer is a
model architecture that deviates from the recurrent pattern (\eg RNNs) and is
solely relying on attention mechanisms. The nature of this architecture makes
the Transformer significantly more easy to parallelize and has as a result a
higher quality of predictions and sequence translations after only being trained
for a small amount of time.

The Transformer's architecture is structured as a \emph{stack of $N$ identical
layers}. Each layer has two main sub-layers. The first is a \emph{multi-head
self-attention mechanism}, and the second is a position-wise fully connected
neural network. The output of each sub-layer is $LayerNorm(x + SubLayer(x))$,
where $SubLayer(x)$ is the function implemented by each sub-layer, followed by a
residual connection around each of the two sub-layers and  by layer
normalization $LayerNorm(x)$. To facilitate these residual connections, all
sub-layers in the model, as well as the input \emph{embedding layers}, produce
outputs of the same dimension $d_{model}$.

This architecture is used as described above in a seq2seq model
\citep{Vaswani_2017} for the encoder. The decoder would require an extra third
sub-layer, which performs multi-head attention over the output of the encoder
stack. However, in our task we need to learn sets of error production rules,
which can be framed as a classic \emph{classification} problem. We are dealing
with input sequences of varied sizes, where no straightforward feature
extraction technique exists. Therefore, we utilize the effectiveness of the
state-of-the-art \emph{Transformer encoder} to summarize the programs into
fixed-length vectors that can then be fed into classic \dnn{} classifiers.

% TODO: Update the paragraph below
% Scaled Dot-Product Attention, computes the attention function on a set of
% queries simultaneously, packed together into a matrix. Multi-head attention,
% allows the model to jointly attend to information from different representation
% subspaces at different positions. A self-attention layer connects all positions
% with a constant number of sequentially executed operations, whereas a recurrent
% layer requires O(n) sequential operations. In terms of computational complexity,
% self-attention layers are faster than recurrent layers when the sequence length
% is smaller than the representation dimensionality, which is often the case with
% machine translations.

\subsection{Transformer Classifier}
\label{sec:seq-classifiers:location-rank}

Our goal with the $\trainDLsym$ function is to train a \emph{classifier} given a
training set $\List{t^a \times \errorrulessym}$ of labeled token sequences.
$\Model$ is used to predict error production rules for a new input program
$t^a$. Critically, we require that the classifier outputs a \emph{confidence
score} $\Conf$ for each possible error rule that measures how sure the
classifier is that the rule can be used to parse the associated input program
$e_{\bot}$. We consider a Transformer encoder for the varied-sized programs and
standard neural networks to generate our model $\Model$.

\mypara{Multi-class \dnn{}s}
%
A \dnn $\Model$ can be used as a binary classifier, \ie predict if the given
input belongs into a class or not. While this model is enough for many tasks,
\eg error localization \citep{Sakkas_2020}, in the case of error rule prediction
we have to select from more \emph{classes}. We therefore use a \dnn for our
error rule prediction $\Model$, but we adjust the output layer to have $N$ nodes
for the $N$ chosen error-rule-classes. For multi-class classification problems
solved with neural networks, usually the \emph{softmax} function is used at
output layer \citep{Goodfellow-et-al-2016, Bishop-book-2006}. Softmax assigns
probabilities to each class that must add up to 1. This additional constraint
speeds up training.

\mypara{Training Multi-class Transformer Classifiers}
%
We use a Transformer encoder to represent an abstracted token sequence $t^a =
(t^a_1, t^a_2, \dots, t^a_n)$ into an abstract vector $V \in \R^k$. The abstract
vector $V$ is then immediately fed into the \dnn as input and the error
production labels are used as the ground truth for training this whole
architecture, \ie the \emph{Transformer classifier}. The binary cross-entropy
loss function is used per class to assign the loss per training cycle.

\mypara{Predicting Error Rules}
%
Our ultimate goal is to be able to predict the small set of error production
rules $\errorrulessym$ that a new unknown program $e_\bot$ with syntax errors
needs to get parsed according to the given grammar $G$ and get repaired.
Therefore, the $\predictDLsym$ function uses the trained $\Model$ to predict all
confidence scores $\List{\errorrulessym \times \Conf}$ for each error production
rule. The $\errorrulessym$ are then sorted based on their predicted confidence
score $\Conf$ and finally the \emph{top-N} rules are returned for
error-correcting parsing. $N$ is small number in the 10s that will give accurate
predictions without making the ECE-Parser too slow, as we discuss next in
\autoref{sec:whole-system}.

% \mypara{The Prediction Algorithm}
% Our algorithm first extracts the machine-learning-amenable dataset $D_{ML}$
% from the program pairs dataset $D$. For each program pair in $D$,
% \textsc{Extract} returns a mapping from the erroneous program's subexpressions
% to features and labels. Then, \textsc{InSlice} keeps only the expressions in
% the the type-error slice and evaluates to a list of the respective feature and
% label vectors, which is added to the $D_{ML}$ dataset. This dataset
% is used by the \textsc{Train} function to generate our predictive $Models$, \ie
% $\Model$ and $\Model$.

% At this point we want to generate a $\predictorsym$ for a new unknown program
% $p$. We perform feature extraction for $p$ with \textsc{Extract}, and use
% \textsc{InSlice} to restrict to expressions in $p$'s type-error slice.
% The result is given by $Data(p)$.

% \textsc{Rank} is then applied to all subexpressions produced by $Data(p)$ with
% $\textsc{Map}$, which will create a mapping of the type
% $\Emap{(\labelsym{\Conf})}$ associating expressions with confidence scores. We
% apply \textsc{Rank} to each feature vector that corresponds to an expression in
% the type-error slice of $p$. These vectors are the first elements of $\tilde{p}
% \in Data(p)$, which are of type $\datasym \times \labelsym{\ConfBin}$. Finally,
% $\predictorsym\ Pr$ is returned, which is used by our synthesis algorithm in
% \autoref{sec:synthesis} to correlate subexpressions in $p$ with their confidence
% scores.
