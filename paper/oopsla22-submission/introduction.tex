
\section{Introduction}
\label{sec:intro}

% Context
Although type and runtime errors have received more research
attention and tool support recently, syntax errors remain important, especially for
programmers with less experience in a language.
Consider the program shown in \autoref{fig:bad-prog}.
% which defines two functions |foo| and |bar|.
% Function |foo| adds the argument variable |a| and returns
% it. Function |bar| calls |foo| with the argument variable |a|, adds another
% number and returns the result.
The programmer has introduced an extra |+| operator after the
|return b| on line 6. This extra |+| should be deleted, as shown in the
developer-fixed program in \autoref{fig:fixed-prog}. This
example presents a \emph{syntax} (or \emph{parse}) \emph{error}. Such parse errors can easily
go unnoticed~\citep{Denny_2012, Ahadi_2018} by programmers when they are part of
bigger programs and may require significant effort to fix~\citep{Kummerfeld2003}.

% Our goal is to use historical data of how programmers have fixed similar errors
% in their programs to automatically and rapidly guide programmers to come up with
% candidate solutions like the one above.

\begin{figure}[h]
\centering
\begin{minipage}[c]{0.48\linewidth}
\begin{ecode}
def foo(a):
  return a + 42

def bar(a):
  b = foo(a) + 17
  return b +
\end{ecode}
\subcaption{A Python program with two functions that manipulate an integer. The second one has a parse error.}
\label{fig:bad-prog}
\end{minipage}%
\hspace{0.02\linewidth}%
\begin{minipage}[c]{0.48\linewidth}
\begin{ecode}
def foo(a):
  return a + 42

def bar(a):
  b = foo(a) + 17
  return b
\end{ecode}
\subcaption{A fixed version for the previous example that has no parse
 errors.}
\label{fig:fixed-prog}
\end{minipage}
\caption{A Python program example with syntax errors (left) and its fix (right).}
\label{fig:example-prog}
\end{figure}

We desire an approach for automatically repairing parse errors in off-the-shelf
programs. An effective solution must apply to programs in real languages
efficiently and must produce repairs that are of high quality. Unfortunately,
the current state of the art is insufficient. First, standard parsing algorithms
are efficient but do not produce repairs. Second, natural language processing
sequence models are efficient but lack accuracy. Third, error correcting parsers
have high quality but are not efficient. \emph{We propose \toolname, an approach
for repairing syntax errors that combines the efficiency of sequence models with
the quality of error correcting parsing.}

% Gap
Standard \emph{parsing algorithms}, which check compliance with syntax rules,
are essential in programming systems. Efficient and expressive parsers are used
in modern programming languages (\eg, LR~\citep{Aho1974} or
Earley~\citep{Earley_1970} parsing). These approaches can accurately locate
syntax errors. However, they often fail to provide effective feedback to humans.
For example, they may only indicate the first syntax error or may produce
messages that are not descriptive enough or are too
verbose~\citep{Kummerfeld2003, Ahadi_2018, VanDerSpek_2005}.
% WRW doesn't understand this claim and does not think it is true without
% more explanation. GCC is push-button, requiring no SE effort.
%
% Additionally, these parsers require a lot of manual effort from the software engineers.

Recent advances in the natural language processing (NLP) research domain
\citep{Sutskever_2014, Hardalov_2018} have produced many automatic approaches
for natural language applications, such as language translation. In particular,
deep neural network (\dnn) \emph{sequence models} can learn underlying patterns
from data and have been shown to be remarkably effective at parsing
\citep{Vinyals2015}. They are able to learn to effectively parse sentences into
their associated (serialized) parse trees. Such approaches have been considered
for automatically parsing and repairing programs~\citep{Ahmed_2021}, but initial
efforts lacked accuracy in real-world contexts.

In programming languages research, \emph{error
correcting parsers} (EC-Parsers) \citep{Aho_1972} use special error production
rules to support parsing programs with syntax errors. These parsers can return
minimal-edit repairs that make the programs parse and can simultaneously fix
multiple locations. Their drawback,
however, is a cubic time complexity with respect to the
input program size and a quadratic complexity with respect to the grammar.
% WRW doesn't see why this next bit is a problem the reader cares about. It
% seems like an internal issue that has the same symptom (bad complexity)
% and is more like "our problem".
%
% Futhermore, there is the added cost of the
% large number of additional production rules these parsers have to consider while
% having an explosion of the internal states by maintaining the cost of each
% possible parse.
Unfortunately, these scalability issues have remained over the
decades~\citep{McLean1996, Rajasekaran2014}, leaving EC-Parsers
impractical for most uses in real-world programming languages.

% Innovation
\mypara{Sequence Classifiers for Error-Correcting Parsing}
In this work, we propose training sequence classifiers for predicting
error
production rules for EC-Parsers to automatically, accurately and
efficiently parse programs with syntax errors. Our approach exploits the data-driven
nature of sequence learning, drawing on a large training dataset of pairs
of syntactically incorrect programs and their fixes.
%
Specifically, to enable the efficient usage of a scalable EC-Parser we decompose
the problem into three steps.
%
First, \emph{learn} the set of error productions rules that fix the dataset of
programs from a corpus of fix edits from historical users.
%
Second, \emph{predict} the small set of error rules for a new erroneous program,
by training sequence multi-class classifiers on abstracted program token
sequences.
%
Third, \emph{parse} a given erroneous program using the predicted error rules,
thus generating a repair.
%
Critically, we show how to perform the abstraction from a particular
program to an abstract token sequence by using an Earley parser's
\citep{Earley_1970} partial parses and a learned Probabilistic Context-Free
Grammar. This abstraction lets us train accurate predictors over low-level
program sequences and thus allows the efficient and high-quality
error-correcting parsing of programs with syntax errors.

% Results
We implemented our approach, \toolname, and trained it and tested it on a
dataset of more than 1,100,000 programs taken from over two years of
user interactions with an online editor. Given a new erroneous program,
\toolname first generates a list of potential error production rules ranked by
likelihood. We evaluate our approach in three ways.
%
First, we measure its \emph{accuracy}: we show that \toolname correctly predicts
the right set of error rules $81\%$ of the time when considering the top $20$
rules and can parse $94\%$ of our tests within $5.3$ seconds with these
predictions.
%
Second, we measure its \emph{efficiency}: we show that \toolname
parses and repairs erroneous programs within $30$ seconds $82\%$ of the time,
while also generating \emph{the user fix in almost 1 out 3 of the cases}.
%
Finally, we measure the \emph{quality} of the generated repairs via a human study
with FIXME participants and show that humans perceive both \toolname's edit
locations and final repair quality to be FIXME in a statistically-significant manner.

The rest of the paper is organized as followed: \autoref{sec:error-analysis}
presents a data analysis of a \python dataset of erroneous programs and their
fixes, which aims to signify the importance of syntax errors in software
development and the need of an efficient automated repair tool.
\autoref{sec:overview} presents a high-level overview of our approach and
\autoref{sec:prog-abstract}, \autoref{sec:seq-classifiers} and
\autoref{sec:whole-system} provide with the specific details that are needed to
implement \toolname. Finally, \autoref{sec:eval} summarizes the quantitative and
qualitative results of our approach and \autoref{sec:related-work} presents
related work in parsing and sequence models.
