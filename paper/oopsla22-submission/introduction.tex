\section{Introduction}
\label{sec:intro}

% Context
Writing syntactically-correct code can be difficult for novice 
programmers. Consider the simple program,
shown in \autoref{fig:bad-prog}, which defines two functions |foo| and
|bar|. 
% Function |foo| adds the argument variable |a| and returns
% it. Function |bar| calls |foo| with the argument variable |a|, adds another
% number and returns the result. 
The programmer has introduced an extra |+| operator after the
|return b| statement on line 6. This extra |+| likely needs to be deleted, as the
programmer intended in the fixed program in \autoref{fig:fixed-prog}. This
example presents a \emph{syntax} (or \emph{parse}) \emph{error}. Such parse errors can easily
go unnoticed~\citep{Denny_2012, Ahadi_2018} by programmers when they are part of
bigger programs and may require significant effort to fix~\citep{Kummerfeld2003}.

% Our goal is to use historical data of how programmers have fixed similar errors
% in their programs to automatically and rapidly guide programmers to come up with
% candidate solutions like the one above.

\begin{figure}[h]
\centering
\begin{minipage}[c]{0.48\linewidth}
\begin{ecode}
def foo(a):
  return a + 42

def bar(a):
  b = foo(a) + 17
  return b +
\end{ecode}
\subcaption{A Python program with two functions that manipulate an integer. The second one has a parse error.}
\label{fig:bad-prog}
\end{minipage}%
\hspace{0.02\linewidth}%
\begin{minipage}[c]{0.48\linewidth}
\begin{ecode}
def foo(a):
  return a + 42

def bar(a):
  b = foo(a) + 17
  return b
\end{ecode}
\subcaption{A possible fixed version for the previous example that has no parse
 errors.}
\label{fig:fixed-prog}
\end{minipage}
\caption{A simple Python program example}
\label{fig:example-prog}
\end{figure}

% Gap
\emph{Parsing algorithms}, which check compliance with syntax rules, are
essential parts of programming systems. Research around these algorithms
has led to 
efficient or expressive parsers used in modern programming languages, \eg 
LR parsing~\citep{Aho1974} or Earley parsing~\citep{Earley_1970}. These highly-engineered
approaches are usually very accurate in locating syntax errors, however, many
times they fail to provide effective feedback to the developers. For example, they may only
point to the first syntax error or may produce messages that are not descriptive enough
or are too verbose~\citep{Kummerfeld2003, Ahadi_2018,
VanDerSpek_2005}. 
% WRW doesn't understand this claim and does not think it is true without
% more explanation. GCC is push-button, requiring no SE effort. 
% 
% Additionally, these parsers require a lot of manual effort from the software engineers.

Recent advances in the Natural Language Processing (NLP) research domain
\citep{Sutskever_2014, Hardalov_2018} have suggested many automatic approaches
for natural language applications, such as language translation. In
particular, \emph{sequence models} can learn underlying
patterns in their data and have been shown to be remarkably
effective at parsing \citep{Vinyals2015}. The vast availability of data has
rendered such deep neural network (\dnn) architectures rather successful. They
are able to learn to effectively parse sentences into their associated
(serialized) parse trees. Such approaches have been considered for
automatically parsing and repairing programs~\citep{Ahmed_2021}, but 
initial efforts lacked accuracy in real-world contexts. 

In programming languages research, \emph{error
correcting parsers} (EC-Parsers) \citep{Aho_1972} use special error production
rules to support parsing programs with syntax errors. These parsers can return
minimal-edit repairs that make the programs parse and can simultaneously fix
multiple locations with minimal effort from the programmers. Their drawback,
however, is that they have a cubic time complexity with respect to the
input program size and a quadratic complexity with respect to the grammar
used. 
% WRW doesn't see why this next bit is a problem the reader cares about. It
% seems like an internal issue that has the same symptom (bad complexity)
% and is more like "our problem". 
% 
% Futhermore, there is the added cost of the
% large number of additional production rules these parsers have to consider while
% having an explosion of the internal states by maintaining the cost of each
% possible parse. 
Unfortunately, these scalability issues have remained over the
decades~\citep{McLean1996, Rajasekaran2014}, leaving EC-Parsers
impractical for most uses in real-world programming languages.

% Innovation
\mypara{Sequence Classifiers for Error-Correcting Parsing}
In this work, we propose training sequence classifiers for predicting error
production rules for EC-Parsers in order to automatically, accurately and
efficiently parse programs with syntax errors. Our approach exploits data-driven
approaches from sequence learning and realizes them by using a large training
dataset of pairs of syntactically incorrect programs and their fixed versions.
%
Specifically, to enable the efficient usage of a scalable EC-Parser we decompose
the problem into three steps:
%
First, \emph{learn} the set of error productions rules that fix the dataset of
programs from a corpus of fix edits that the users made.
%
Second, \emph{predict} the small set of error rules for a new erroneous program,
by training sequence multi-class classifiers on abstracted program token
sequences.
%
Third, \emph{parse} the given erroneous program with the predicted error rules,
thus generating a repair.
%
Critically, we show how to perform the crucial abstraction from a particular
program to an abstract token sequence by using an Earley parser's
\citep{Earley_1970} partial parses and a learnt Probabilistic Context-Free
Grammar. This abstraction lets us train accurate predictors over low-level
program sequences allowing the efficient error-correcting parsing of programs
with syntax errors.

% Results
\mypara{\toolname}
We implemented our approach in \toolname and trained it and tested it on a
dataset of more than 1,100,000 programs. Given a new erroneous program,
\toolname generates a list of potential error production rules ranked by
likelihood. We train \toolname on programs from a online editor from over two
years and evaluate in several ways.
%
First, we measure its \emph{accuracy}: we show that \toolname correctly predicts
the right set of error rules $81\%$ of the time when considering the top $20$
rules and can parse $92\%$ of our tests within $4.1$ seconds with these
predictions.
%
Second, we measure its \emph{efficiency}: we show that \toolname is able to
parse and repair erroneous programs within $25$ seconds $81\%$ of the time,
while also generating \emph{the user fix in almost 1 out 3 of the cases}.
%
Finally, we measure the \emph{quality} of the generated repairs via a user study
with XX participants and show that humans perceive both \toolname's edit
locations and final repair quality to be better than YY in a
statistically-significant manner.
