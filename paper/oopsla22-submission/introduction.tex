\section{Introduction}
\label{sec:intro}

% Context
Writing syntactically correct code is a difficult process that usually requires
some programming experience from the programmer. Consider the simple program
shown in \autoref{fig:bad-prog}. The program defines two functions |foo| and
|bar|. Function |foo| adds some number to the argument variable |a| and returns
it. Function |bar| calls |foo| with the argument variable |a|, adds another
number and returns the result. However, there is an extra |+| operator after the
|return b| statement. The extra |+| mostly likely needs to be deleted, as the
programmer intended, as shown in the fixed program in \autoref{fig:fixed-prog}.
This presents a \emph{syntax (or parse) error}. Such parse errors can easily go
unnoticed \citep{Denny_2012, Ahadi_2018, VanDerSpek_2005} by programmers when
they are part of bigger programs and may still require significant effort to fix
them \citep{Kummerfeld2003}.

% Our goal is to use historical data of how programmers have fixed similar errors
% in their programs to automatically and rapidly guide programmers to come up with
% candidate solutions like the one above.

\begin{figure}[h]
\centering
\begin{minipage}[c]{0.48\linewidth}
\begin{ecode}
def foo(a):
  return a + 42

def bar(a):
  b = foo(a) + 17
  return b +
\end{ecode}
\subcaption{A Python program with two functions that manipulate an integer. The second one has a parse error.}
\label{fig:bad-prog}
\end{minipage}%
\hspace{0.02\linewidth}%
\begin{minipage}[c]{0.48\linewidth}
\begin{ecode}
def foo(a):
  return a + 42

def bar(a):
  b = foo(a) + 17
  return b
\end{ecode}
\subcaption{A possible fixed version for the previous example that has no parse
 errors.}
\label{fig:fixed-prog}
\end{minipage}
\caption{A simple Python program example}
\label{fig:example-prog}
\end{figure}

% Gap
On the one hand, \emph{parsing algorithms} are essential parts of programming
languages. A lot of research has been done around these algorithms and many
efficient parsers have been used in modern programming languages, \eg LR parsing
\citep{Aho1974} or Earley parsing \citep{Earley_1970}. These highly engineered
approaches are usually very accurate in locating syntax errors, however, many
times they fail to provide valuable feedback to the developers, \eg they only
point to the first syntax error or the error messages are not descriptive enough
or have too much information \citep{Kummerfeld2003, Ahadi_2018}. Additionally,
these parsers require a lot of manual effort from the software engineers.

Recent advances in the Natural Language Processing (NLP) research domain
\citep{Sutskever_2014, Hardalov_2018} have suggested many automatic approaches
for natural language applications, such as language translation. They have made
significant progress by utilizing \emph{sequence models} to learn underlying
patterns in their data. These algorithms have also been shown to be remarkably
effective at parsing \citep{Vinyals2015}. The vast availability of data has
rendered these deep neural network (\dnn) architectures rather successful. They
are able to learn to effectively parse sentences to their associated
(serialized) parse trees. Such approaches have been applied on automatically
parsing and repairing programs \citep{Ahmed_2021}, but they, however, are still
not very accurate to parse real-world programs.

Relevant work in programming languages research has suggested \emph{error
correcting parsers} (EC-Parsers) \citep{Aho_1972}, that uses error production
rules for parsing programs with syntax errors. These parsers can return
minimal-edit repairs that make the programs parse and can simultaneously fix
multiple locations with minimal effort from the programmers. Their drawback,
however, have a time complexity that is cubed on the input program size and
squared on the grammar size used. Futhermore, there is the added cost of the
large number of additional production rules these parsers have to consider while
having an explosion of the internal states by maintaining the cost of each
possible parse. Additionally, barely any improvements have ever been introduced
\citep{McLean1996, Rajasekaran2014} which renders EC-Parsers impractical for use
in real-world programming languages.

% Innovation
\mypara{Sequence Classifiers for Error-Correcting Parsing}
In this work, we propose training sequence classifiers for predicting error
production rules for EC-Parsers in order to automatically, accurately and
efficiently parse programs with syntax errors. Our approach exploits data-driven
approaches from sequence learning and realizes them by using a large training
dataset of pairs of syntactically incorrect programs and their fixed versions.
%
Specifically, to enable the efficient usage of a scalable EC-Parser we decompose
the problem into three steps:
%
First, \emph{learn} the set of error productions rules that fix the dataset of
programs needs from a corpus of fix edits that the users made.
%
Second, \emph{predict} the small set of error rules for a new error program, by
training sequence multi-class classifiers on the abstracted program token
sequences.
%
Third, \emph{parse} the given erroneous program with the predicted error rules,
thus generating a repair.
%
Critically, we show how to perform the crucial abstraction from a particular
program to an abstract token sequence by using the an Earley parser's
\citep{Earley_1970} partial parses and a learnt Probabilistic Context-Free
Grammar. This abstraction lets us train accurate predictors over low-level
program sequences allowing the efficient error-correcting parsing of programs
with syntax errors.

% Results
\mypara{\toolname}
We implemented our approach in \toolname and trained it and tested it on a
dataset of more than 1,100,000 programs. Given a new erroneous program,
\toolname generates a list of potential error production rules ranked by
likelihood. We train \toolname on programs from a online editor from over two
years and evaluate in several ways.
%
First, we measure its \emph{accuracy}: we show that \toolname correctly predicts
the right set of error rules $81\%$ of the time when considering the top $20$
rules and can parse $92\%$ of our tests within $4.1$ seconds with these
predictions.
%
Second, we measure its \emph{efficiency}: we show that \toolname is able to
parse and repair erroneous programs within $25$ seconds $81\%$ of the time,
while also generating \emph{the user fix in almost 1 out 3 of the cases}.
%
Finally, we measure the \emph{quality} of the generated repairs via a user study
with XX participants and show that humans perceive both \toolname's edit
locations and final repair quality to be better than YY in a
statistically-significant manner.
