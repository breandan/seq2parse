\section{Evaluation}
\label{sec:eval}

We have implemented our approach in \toolname: a system for repairing parse
errors for \python at its entirety. Next, we describe our implementation and an
evaluation that addresses four questions:

\begin{itemize}
    \item \textbf{RQ1}: How \emph{accurate} are \toolname's predicted error production rules?
                        (\S~\ref{sec:eval:accuracy})
    \item \textbf{RQ2}: How \emph{precisely} can \toolname repair parse errors?
                        (\S~\ref{sec:eval:precise})
    \item \textbf{RQ3}: How \emph{efficiently} can \toolname repair parse errors?
                        (\S~\ref{sec:eval:efficiency})
    \item \textbf{RQ4}: How \emph{useful} are \toolname's suggested repairs?
                        (\S~\ref{sec:eval:useful})
\end{itemize}

% \subsection{Implementation} \label{sec:eval:gen_method}

\mypara{Training Dataset}
For our evaluation, we use the same \python dataset that we used in our error
data analysis in \autoref{sec:error-analysis} gathered from
PythonTutor.com~\citep{Guo2013} between the years 2017 and 2018. The dataset has
more than 1,100,000 usable erroneous Python programs and their respective fixes.
The programs have an average length of \emph{87 tokens}, while the abstracted
token sequences have a much shorter average of \emph{43 tokens}. We choose
15,000 random programs from the dataset for all our tests, and the rest we use
as our training set.

We first learn a PCFG on the training set of fixed programs to learn the
probabilities for each production rule in the \emph{full \python grammar}.
\toolname then extracts the abstracted token sequences for all programs in the
training set. Next, while the full \python grammar has \emph{455 possible
terminal error production rules}, in reality, only \emph{340 error rules} are
ever used in our dataset and are assigned as \emph{labels}. We arrive at this
set of error rules by parsing all the erroneous programs in the training set
with the ECE-Parser and the ``diff'' error rules, as described in
\autoref{sec:whole-system:error-rules}.

\mypara{Transformer Classifier}
\toolname's error rule prediction uses a Transformer classifier with \emph{six}
transformer blocks, that each has a fully-connected hidden layer of 256 neurons
and 12 attention heads. The output of the transformer blocks is then connected
to a \dnn based classifier with \emph{two} fully-connected hidden layers of 256
and 128 neurons respectively. The neurons use rectified linear units (ReLU) as
their activation function , while the output layer uses the sigmoid function for
each class. Additionally, there are \emph{two input embedding layers} of a length
of 128, one for input tokens and one for their positions in the sequence. We
also limit the input abstracted token sequences to a length of 128 units, which
covers $95.7\%$ of the training set, without the need of pruning them. Finally,
the Transformer classifier was trained using an \textsc{Adam} optimizer
\citep{Kingma2014-ng}, a variant of stochastic gradient descent, for a total of
50 epochs.

\subsection{RQ1: Accuracy}
\label{sec:eval:accuracy}

\input{evaluation-classifier-accuracy}

% \mypara{Results: Accuracy of Error Rule Prediction}
%
\autoref{fig:accuracy-results} shows the accuracy results of our error
production rule prediction experiments. The y-axis describes the
\emph{prediction accuracy}, \ie the fraction of test programs for which the
correct \emph{full set} of error rules to repair the program was predicted in
the top-K sorted rules.
%
The \textsc{Original} version of our transformer classifier does not consider
the abstracted token sequences and used the full \textsc{Original} token
sequences, whose results are presented in the first two bars of
\autoref{fig:accuracy-results}. The next two bars show our final results using
the \textsc{Abstracted} token sequences to train the classifier. Finally, the
last two dotted bars show the results for when a probability \textsc{Threshold}
is set in order to select the predicted error rules (instead of picking the
static top-K ones) but using again the \textsc{Abstracted} sequences as input.
The predicted error rule set can have a size anywhere between 1 and a maximum of
20 rules.

The blue bars show the accuracy on the full test set of \textsc{All} 15,000 test
programs, while the green bars show the results on a subset of \textsc{Rare}
programs, \ie programs that did not include any of the 50 most popular error
rules. The \textsc{Rare} programs amount only for 4\% of our test set.

The \textsc{Original} predictor, even with the Top-50 predicted error rules is
less accurate than the Top-20 predictions of the \textsc{Abstracted}, with an
accuracy of 87.13\%, which drops to 68.48\% and 56.71\% respectively for the
Top-20 and Top-10 predictions. The \textsc{Abstracted} predictor significantly
outperforms the \textsc{Original} predictor with a 72.11\% Top-10 accuracy,
81.45\% Top-20 accuracy and 92.70\% Top-50 accuracy.

The \textsc{Threshold} predictions are almost as accurate as the
\textsc{Abstracted} Top-20 predictions with an accuracy of 79.28\% and a median
number of selected error rules of 14 (average 14.1). This could potentially mean
that this predictor is a valid alternative for the static Top-20 predictions.

Finally, we observe that our \textsc{Abstracted} classifiers generalize
efficiently for our dataset of erroneous \python programs and is almost as
accurate for the \textsc{Rare} programs as the rest of the dataset with a
73.32\% Top-20 accuracy (88.81\% Top-50 accuracy). The same holds for the
\textsc{Threshold} predictions with a 69.83\% \textsc{Rare} accuracy.

\begin{framed}
  \noindent \toolname's transformer classifier learns to encode programs with
  syntax errors and select candidate error production rules for them
  effectively, yielding \emph{high accuracies}. By abstracting the tokens
  sequences, \toolname is able to \emph{generalize} better and make more
  accurate predictions with a \emph{81.45\% Top-20 accuracy}.
\end{framed}


\subsection{RQ2: Repaired Program Preciseness}
\label{sec:eval:precise}

Next we evaluate \toolname's end-to-end accuracy and preciseness in generating
valid parses for programs with syntax errors. For all of our tests we limit
\toolname's parsing time to \emph{5 mins} and run our experiments on the
15,000-program test set. Additionally, we use here the highest-performing
transformer classifiers, \ie the \textsc{Abstracted} and \textsc{Threshold}
classifiers.

We compare \emph{three versions} of our \toolname implementation
(\textsc{AllParses}, \textsc{MinimumCost} and \textsc{Threshold}) against two
versions of the EC-Parser with a static selection of the 20 and 50 most popular
error production rules in our training set. We make this choice because we
observe that the 50 most popular error rules are used as labels for as much as
\emph{86\%} of the training set.

Our \textsc{AllParses} ECE-Parser and \textsc{MinimumCost} ECE-Parser both use
the \emph{Top-20 predictions} from our \textsc{Abstracted} classifier to parse
and repair buggy programs. The \textsc{AllParses} ECE-Parser keeps internally
\emph{all possible states} that arise from using the predicted error rules
similarly to the original ECE-Parser described in \citep{Aho_1972}. We use a
maximum repair cost of 2 edits (\ie a maximum of 2 insertions, deletions or
replacements) to limit the search space. The \textsc{MinimumCost} version
however keeps always the minimum-edit repair and discards all other states that
may lead to a higher cost. This allows for a higher maximum cost of 10 edits. We
use the same ECE-Parser and cost as in \textsc{MinimumCost} for our
\textsc{MinimumCost} parser. Finally, while \textsc{AllParses} can generate a
large number of repairs, we keep only the top 3 repairs after filtering with a
static code checker (\textsc{Pylint}, \url{https://www.pylint.org/}) as most
developers will consider only a few suggestions before falling back to manual
debugging \citep{Kochhar2016-oc, Parnin2011-ce}.

\autoref{tab:seq2parse_full_results} shows the percentage of test programs that
each of these four versions can parse successfully (\ie the parse accuracy), the
rare programs parse accuracy, and the amount of parses that match the one that
the user compiled. We observe that the \textsc{MinimumCost} parser
\emph{outperforms} every other option with 94.25\% parse accuracy and 94.01\%
rare parse accuracy. It also generates the intended user parse for 20.55\% of
the set, \ie over 1 out 5 of the cases. The 20 most popular parser with 79.87\%
parse accuracy and 65.01\% rare parse accuracy is much less accurate, and is
4.24\% less likely to generate the user parse, while the 50 most popular is
slightly less accurate with 90.89\% and 81.26\% accuracy, as expected from the
usage of a large number of popular error rules. The 50 most popular parser has
also a high user fix accuracy of 18.56\%. The \textsc{AllParses} parser has a
lower parse accuracy of 61.82\%, however it manages to generate the user fix
33.92\% of the time and achieve a high 72.22\% rare accuracy. Finally, the
\textsc{Threshold} parser is almost as accurate as the efficient
\textsc{MinimumCost} parser with 94.19\% parse accuracy and 93.42\% rare parse
accuracy, while achieving a slightly higher user fix accuracy of 21.19\%.

\begin{framed}
  \noindent \toolname can \emph{parse and repair 94.25\%} of programs with
  syntax errors. In addition, it generates \emph{the exact user fix over 20\% of
  the time}.
\end{framed}

\begin{figure}[t]
  \centering
  \begin{minipage}[c]{0.46\linewidth}
    \centering
    \resizebox{\linewidth}{!}{
      \begin{tabular}{l||ccc}
        Error Rule            & \emph{Parse}     & \emph{Rare Parse} & \emph{User Fix} \\
        Approach              & \emph{Accuracy}  & \emph{Accuracy}   & \emph{Accuracy} \\
        \hline
        20 Most Popular       & 79.87\%          & 65.01\%           & 16.31\% \\
        50 Most Popular       & 90.89\%          & 81.26\%           & 18.56\% \\
        \textsc{AllParses}    & 61.82\%          & 72.22\%           & \textbf{33.92\%} \\
        \textsc{MinimumCost}  & \textbf{94.25\%} & \textbf{94.01\%}  & 20.55\% \\
        \textsc{Threshold}    & 94.19\%          & 93.42\%           & 21.19\% \\
      \end{tabular}
    }
    \caption{Experimental results of \toolname's repair approaches.}
    \label{tab:seq2parse_full_results}
  \end{minipage}
  \begin{minipage}[c]{0.53\linewidth}
    \centering
    \includegraphics[width=\linewidth]{tool-repair-rate.pdf}
    \caption{The repair rate for all the approaches in
    \autoref{tab:seq2parse_full_results}}
    \label{fig:tool-repair-rate}
  \end{minipage}
\end{figure}

\subsection{RQ3: Efficiency}
\label{sec:eval:efficiency}

% \begin{figure}[t]
%   \centering
%   \includegraphics[width=0.8\linewidth]{tool-repair-rate.pdf}
%   \caption{The repair rate for all the approaches in
%   \autoref{tab:seq2parse_full_results}}
%   \label{fig:tool-repair-rate}
% \end{figure}

Next we evaluate \toolname's efficiency by measuring how many programs it is
able to parse. We limit each ECE-Parser to 5 minutes. (In general the procedure
is undecidable, and we conjecture that a longer timeout will diminish the
practical usability for developers.) We compare the efficiency of \toolname for
all the versions of \autoref{tab:seq2parse_full_results}.

\autoref{fig:tool-repair-rate} shows the cumulative distribution function of all
\toolname approaches' repair rates over their repair time. We observe that using
\textsc{Threshold} predictions with the \textsc{MinimumCost} ECE-Parser is the
most efficient and it maintains a highest parse accuracy at all times, with a
repair rate of 83.04\% within 20 seconds and a median parse time of 2.1 seconds.
The \textsc{MinimumCost} with the top 20 error rule predictions is still very
efficient with a repair rate of 78.10\% within 20 seconds and a median parse
time of 5.3 seconds.

We observe that, using a fixed set of the 20 and 50 most popular rules for
\toolname to parse programs with syntax errors (with the \textsc{MinimumCost}
ECE-Parser), repairs 61.41\% and 58.61\% of the programs respectively within 20
seconds, and have median parse times of 7.0 and 13.6 seconds respectively. The
50 most popular ECE-Parser parses less programs until 28 seconds than the 20
most popular ECE-Parser, but the extra number of error rules aids the ECE-Parser
to parse programs that the other version couldn't.

We also observe that \toolname successfully parses around 38.50\% of the
programs with its \textsc{AllParses} approach in 20 seconds and has a median
parse time of 23.2 seconds. While this approach is much less efficient that the
others, due to the vast amount of state in internal data
structure, it is also able to generate the exact human repair in 1 out of 3
cases, representing a valuable quality tradeoff (\S~\ref{sec:eval:precise}).

\begin{framed}
  \noindent \toolname can parse programs with syntax errors for the vast
  majority of the test set in under 20 seconds with a median parse time of 2.1
  seconds.
\end{framed}

\subsection{RQ4: Usefulness}
\label{sec:eval:useful}

% FIXME - define word for non-equivalent repairs

As \toolname is intended as an aid for programmers (especially novices) faced with parse errors, we
are also interested subjective human judgments of the quality and
helpfulness of our repairs. Around 34\% of repairs produced by \toolname using
its \textsc{AllParses} approach are identical to the historical human repair and
thus likely helpful for programmers. However, it may be that \toolname's parses
(and thus repairs) are still helpful for debugging even when they differ slightly from
the human repair (\ie \textit{non-equivalent} repairs). To investigate this
hypothesis, we conduct a human study of the quality and debugging helpfulness of
\toolname's non-equivalent repairs.

\mypara{Human Study Setup} We recruited participants from two large public
research institutions (names omitted for blind-review) and through Twitter.
The study was online, took around 30 minutes, and participants could enter a
drawing for one of two \$50 awards. In the study, participants
were each asked to rate 15 debugging hints randomly selected from a corpus of 50
stimuli.\footnote{All human study stimuli are included in our replication
package at \emph{link removed for blind review}.}

We created the stimuli by selecting 50 buggy programs from our test set for
which \toolname and the human produced different fixes. Other than ensuring a
wide array of difficulty (as assessed by how long the human took to fix the
error), programs were selected randomly. Each stimulus consisted of a buggy
program, its associated syntax error message, and a potential program fix
presented as a \emph{debugging hint}. For each stimulus, we produced two
versions: one where the debugging hint was generated by \toolname and one where
the debugging hint was the historical human fix. Note that, in practice, the
historical human fix would \emph{not} be available to a struggling novice
in real situations: it represents future or oracular information. Informally, in
our comparison the historical human fixes can be viewed as an upper bound.

Participants rated the quality and
helpfulness of each debugging hint using a 1--5 Likert scale. They also indicated
if the debugging hint provided helpful information beyond that in the Python
error message.
%\footnote{In this study, we used error messages from Python 3.10. Compared to earlier Python versions, 3.10 includes improved error messages for Syntax Errors.}.
Participants were unaware of whether any given hint was generated by a human or
\toolname, and participants were never shown both the tool-generated and human
fixes to the same buggy program. To be included in the analysis, participants
had to assess at least four stimuli. Overall, we analyze 527 unique stimuli
ratings from $n=39$ valid participants (246 for human fixes and 281 for
\toolname repairs).

\mypara{Overall Results} While humans in our study find that non-equivalent repairs produced by
\toolname are lower in both quality and debugging helpfulness than those produced
manually (2.9/5 helpfulness for tool-produced repairs vs. 3.7/5 for human-produced
repairs, $p < 0.001$), humans still often find \toolname's fixes
helpful for debugging: participants found that \toolname repairs contained helpful
debugging information beyond that contained in the Python Error message 48\% of
the time (134/281). This additional debugging information was helpful in terms of both
the content (73\% of the time) and location (55\% of the time).
Additionally, \toolname fixes are helpful for easy and hard Syntax Errors
alike: we found no statistically-significant difference between the helpfulness or
quality of \toolname's repairs for easy (those repaired by the human in under
40 seconds) or hard parse errors (over 40 seconds).
%p = 0.07 and 0.13 respectively if we have room to include
Overall, these results indicate that even when \toolname repairs differ from
historical human repairs, they can still be helpful for debugging.

\mypara{Individual Stimuli} Beyond an analysis of \toolname's overall quality, we also
analyze the helpfulness of each stimulus. % FIXME: If room, can add more statistical details if needed
Of the 48 programs for which we collected sufficient data to permit statistical
comparison, the historical repair was statistically more helpful for debugging than \toolname's
repair for 33\% of stimuli (16/48, $p<0.05$). However, we found that \toolname's
repair was actually \emph{more helpful} for debugging than the human's repair for 15\% of
stimuli (7/48, $p<0.05$). For the remaining 52\% of stimuli, we found no evidence of a
statistical difference in the debugging helpfulness of the two repairs.

\begin{figure}[h]
  \centering
  \begin{minipage}[c]{0.47\linewidth}
  \begin{python2}
# Buggy
def gcdIter(a, b):
    for i in range(1, a+1):
        if a % i == 0:
        elif b % i == 0:
    return i
gcdIter(9, 12)
  \end{python2}
  \begin{python2}
# Human
def gcdIter(a, b):
    for i in range(1, a+1):
        return a % i
gcdIter(9, 12)
  \end{python2}
  \begin{python2}
# Seq2Parse
def gcdIter(a, b):
    for i in range(1, a+1):
        if a % i == 0: new_var
        elif b % i == 0: break
    return i
gcdIter(9, 12)
  \end{python2}
  \subcaption{\toolname repair significantly more helpful:
  4.3/5 vs 1.0/5, $p = 0.03$}
  \label{fig:proga}
  \end{minipage}%
  \hspace{0.02\linewidth}%
  \begin{minipage}[c]{0.47\linewidth}
    \begin{python2}
aList = [12, 'yz', 'ab'];
aList.reverse();
print "List : ", aList
    \end{python2}
    \begin{python2}
aList = [12, 'yz', 'ab']
aList.reverse()
    \end{python2}
    \begin{python2}
aList = [12, 'yz', 'ab']
aList.reverse()
print("List : ", aList)
    \end{python2}
    \subcaption{\toolname repair significantly more helpful:
    4.75/5 vs 2.0/5, $p = 0.02$}
    \label{fig:progb}

    \begin{python2}
a = int(input(enter a))
print(a***3)
    \end{python2}
    \begin{python2}
a = int(input("enter a"))
print(a**3)
    \end{python2}
    \begin{python2}
a = int(input(enter)(a))
print(a ** (* 3))
    \end{python2}
    \subcaption{Historical human repair significantly more helpful:
    1.8/5 vs 4.75/5, $p = 0.01$}
    \label{fig:progc}
    \end{minipage}%

  \caption{Three example buggy programs followed by their historical human
  and \toolname repairs. For (a) and (b), \toolname's repair
  was rated more helpful by participants. For (c), the human
  repair was more helpful.}
  \label{fig:user_study_examples}
\end{figure}

To better contextualize these results, we provide examples of stimuli with
statistically significant differences in debugging helpfulness.
In figure~\ref{fig:progb}, \toolname's repair was
significantly more helpful than the historical repair:
 \toolname correctly adds parentheses to \texttt{print}
while the human simply deletes the
buggy line, perhaps out of confusion or frustration.
Similarly, figure~\ref{fig:proga}'s \toolname repair was also
better than the human repair. In this case, the user appears to try to
implement a function to calculate the greatest common divisor of two integers,
but has empty \texttt{if} and \texttt{elif} statements.
To ``fix'' this bug, the user
deletes the \texttt{if} and \texttt{elif} and modifies the return statement.
However, this fix does not correctly calculate the greatest common divisor.
\toolname, on the other hand, adds a template variable to the \texttt{if} and
\texttt{break} to the \texttt{elif}. While this also does not implement
greatest common divisor, it is viewed as more helpful
than the user repair. This example also demonstrates the beneficial ability of our approach
to conduct multi-edit repairs.

Figure~\ref{fig:progc}, on the other hand, shows an example of a more
helpful human repair. In this case, the
human correctly deletes the extraneous \texttt{*} in the power operator
while \toolname adds parentheses to make a more complex expression, the
result of favoring one insertion over one deletion.

\begin{framed}
  \noindent
  34\% of \toolname's repairs are equivalent to historical repairs.
  Of the remainder, our human study found 15\% to be more useful than
  historical repairs and 52\% to be equally useful. In total, including
  both equivalent and non-equivalent cases, \toolname repairs are at
  least as useful as historical human-written 78\% of the time.

  % Even when non-equivalent, \toolname's repairs are still useful and helpful:
  % they contain debugging information beyond that in the Python error message 48\% of the time.
  % Ultimately, two-thirds of \toolname's fixes were either indistinguishable
  % from (34\%), or judged better than (15\%), those produced by humans for helping debugging.
\end{framed}
