\section{Building a fast error-correcting parser}
\label{sec:whole-system}

We show how $\systemsym$ uses the abstracted token sequences from
\autoref{sec:prog-abstract} and the trained sequence models from
\autoref{sec:seq-classifiers} to generate an \emph{error-correcting parser}
$(e_{\bot} \to e)$, that will parse an input program $e_{\bot}$ with syntax
errors and produce a correct program $e$. We first describe how we extract a
machine-learning-amendable trainings set from a corpus of fixed programs and
finally how we structure everything to train our models.


\subsection{Learning Error Production Rules}
\label{sec:whole-system:error-rules}
We implement $\diffsym$, a function that extracts the \emph{token differences}
between an erroneous program $e_{\bot}$ and a fixed program $e$. $\diffsym$
utilizes an \emph{$O(ND)$ difference algorithm} \citep{Myers_1986} to extract
the \emph{insertion, deletion,} and \emph{replacement} edits to the original
program $e_{\bot}$ that produce the fixed program $e$. We then map the extracted
edits to the relevant error production rules that can be used to repair
$e_{\bot}$.

% \mypara{Extracting error rules efficiently from a dataset}
%
The $\trainDLsym$ method requires a dataset of token sequences $t^a$ that is
annotated with an \emph{exact and small set} of error production rules, \ie
$\List{t^a \times \errorrulessym}$. These error productions are just a subset of
all the possible $\errorrulessym$ that are needed to parse and fix $t^a$. The
straight-forward approach is to use the ECE-Parser $\ecepsym$ with all possible
error production rules for each program $e_{\bot}$ in the dataset. Then, when
$\ecepsym$ returns with a successful parse we extract the rules that where used
to parse program $e_{\bot}$. This approach would generate a dataset with the
smallest possible set of error rules per program as labels, since the original
ECE-Parser returns the minimum-distance edit parse. However, this approach
completely ignores the programmer's fix $e$ and can take an unreasonable amount
of time to parse a dataset with millions of programs, due to the inefficient
nature of the ECE-Parser.

We suggest using an $O(ND)$ difference algorithm to get a small but still
representative set of error production rules for each program $e_{\bot}$. We
employ this algorithm to find the differences between the input
\emph{program token sequence} $t^i$, which is the lexed program $e_{\bot}$ and
the \emph{fixed token sequence} $t^o$, which is the lexed program $e$. This
algorithm returns changes between the token sequences in the form of
\emph{inserted or deleted tokens}. It is possible that this algorithm returns a
sequence of deletions followed by a sequence of insertions, which can in turn
interpreted as a single \emph{replacement} of tokens. We map these three types
of changes to the respective error production rules. Let $t^i$ be a sequence
$t^i_1, t^i_2, \dots, t^i_n$ and $t^o$ be the updated sequence $t^o_1, t^o_2,
\dots, t^o_k$. We map:
\begin{itemize}
    \item an inserted output token $t^o_j$ to a \emph{deletion} error $E_{t^o_j}
    \rightarrow \epsilon$.
    \item a deleted input token $t^i_k$ to an \emph{insertion} error $Tok\
    \rightarrow t^i_k$ and the helper rule $E_{t^i_{k+1}} \rightarrow Ins\
    t^i_{k+1}$.
    \item a replaced token $t^i_k$ with $t^o_j$ to a \emph{replacement} error
    $Repl\ \rightarrow t^i_k$ and the helper rule $E_{t^o_j} \rightarrow Repl$.
\end{itemize}

In the case of an insertion error, we also include the helper rules $Ins\
\rightarrow\ Tok\ \vert\ Ins\ Tok$, that can derive any nonempty sequence of
insertions. To introduce (possible) insertion errors at the end of a program, we
include the production rules $S' \rightarrow S$ and $S' \rightarrow S\ Ins$.

The above algorithm, so far, adds only the \emph{terminal error productions}. We
have to include the \emph{non-terminal error productions} that will invoke the
terminal ones. If $X \rightarrow a_0b_0a_1b_1 \dots a_mb_m,\ m \geq 0$, is a
production in $P$ such that $a_i$ is in $N^*$ and $b_i$ is in $\Sigma$, then we
add the error production $X \rightarrow a_0X_{b_0}a_1X_{b_1} \dots a_mX_{b_m},\
m \geq 0$ to $P'$, where each $X_{b_i}$, is either a new non-terminal $E_{b_i}$
that was added with the above algorithm, or just $b_i$ again if it was not
added.

Finally, we further refine the new small set of error productions for each
program $e_{\bot}$ with ECE-Parser, in order to create the final annotated
dataset $\List{t^a \times \errorrulessym}$. The changes that we extracted from
the programmers' fixes might include irrelevant changes to the parse error fix,
\eg code clean-up. Therefore, running ECE-Parser is still essential to annotate
each program with the appropriate error production rules.
% However, limiting the error rules to only the extracted small set instead of
% every possible error rule, makes the process of generating this dataset a much
% more tractable and efficient process.


\subsection{Training and Using a Transformer Classifier}
\label{sec:whole-system:training-classifier}

\begin{figure}[t]
  \centering
  \begin{minipage}[t]{0.49\linewidth}
    \centering
    \input{training-classifier-algo.tex}
  \end{minipage}
  \hspace{0.02\linewidth}
  \begin{minipage}[t]{0.47\linewidth}
    \centering
    \input{predict-algo.tex}
  \end{minipage}
\end{figure}

% \mypara{Training the Transformer Classifier}
%
We describe in \Cref{algo:training-classifier-algo} how we use a
(probabilistic) grammar $G$ and a dataset $Ds$ to train a Transformer classifier
$Model$ that can be used to predict error production rules for erroneous
programs $\pbad$. We define the $\trainsym$ procedure in
\Cref{algo:training-classifier-algo} that is responsible for extracting a
machine-learning appropriate dataset $D_{ML}$ in order to train the Transformer
classifier with $\trainDLsym$.

The dataset $D_{ML}$ starts as an empty set. For each program pair $\pbad \times
\pfix$, we, first, employ $\partialsym$ with the PCFG $G$ and the erroneous
program $\pbad$ to extract the abstracted token sequence $t^a$. Second, we use
the token difference algorithm $\diffsym$ to extract the specific error $rules$
that fix $\pbad$ based on $\pfix$. The abstracted sequence $t^a$ is annotated
with the label $rules$ and is added to $D_{ML}$. Finally, the Transformer
classifier $Model$ is then learned by $\trainDLsym$ that uses the newly
extracted dataset $D_{ML}$ and is returned by the algorithm. The $\trainsym$
with the $\partialsym$ procedure. Finally, the $\predictDLsym$ procedure can is
performed offline and thus won't affect the performance of the final repair.

% \mypara{Predicting Error Rules}
%
Having trained the Transformer classifier $Model$, we can now predict error
rules $Rls$, that will be used by an ECE-Parser, by using the $\predictsym$
procedure defined in \Cref{algo:predict-algo}. $\predictsym$ uses the same input
grammar $G$ to generate an abstracted token sequence $t^a$ for the program $P$
with the $\partialsym$ procedure. Finally, the $\predictDLsym$ procedure
predicts a small set of error production rules $Rls$ for the sequence $t^a$
given the pre-trained $Model$.

\subsection{Generating an efficient error-correcting parser}
\label{sec:whole-system:building-ecp}

\input{ecep-algo.tex}

We now describe our approach for $\systemsym$ in \Cref{algo:ecep-algo}. This
is the high-level algorithm that combines everything that we described so far in
the last three sections.

$\systemsym$ first extracts the fixed programs $ps$ from the dataset $Ds$ to
learn a probabilistic context-free grammar $PCFG$ for the input grammar $G$.
Then it $\trainsym$s the $Model$ to predict error production rules. Then we
define an error rule predictor function \textsc{ERulePredictor} using
$\predictsym$ with the pre-trained $Model$ and grammar $PCFG$. Finally, the
algorithm returns the error correcting parser $Prs$, which we define as a
function that takes as input an erroneous program $\pbad$ that uses
\textsc{ERulePredictor} to get the set of error rules needed by $\ecepsym$ to
parse and repair it.
