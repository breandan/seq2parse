\section{Overview}
\label{sec:overview}

We begin with an overview of our approach to parsing and repairing programs with
parse errors by fine-tuning error correcting parsers with prior knowledge of the
processes (novice) programmers follow to repair their errors.

\begin{figure}[t]
  \centering
  \includegraphics[trim={0 3.8cm 0 3.8cm}, clip, width=0.9\linewidth]{overall-approach.pdf}
  \caption{\toolname's overall approach.}
  \label{fig:overall-approach}
\end{figure}

\mypara{Error Correcting Parsers}
One approach to fixing erroneous programs (as in \autoref{fig:bad-prog})
is to use \emph{Error Correcting Parsers} (EC-Parsers) \citep{Aho_1972}.
EC-Parsers are based on dynamic programming algorithms and have been used
to automatically infer the intended parse trees for programs
that have syntax errors. These dynamic programming parsers have been used
with \emph{error correcting production rules} \citep{Aho_1972} to
generate a valid parse by making minimal edits to the original program.
However, EC-Parsers are very inefficient for larger
grammars, since they require the addition of a large number of error production
rules, making them impractical for real-world applications. In contrast, the
original Earley parsing algorithm is also based on dynamic programming
but does not directly support error correction~\citep{Earley_1970}.

\mypara{Solution: Train Classifier to Predict Error Rules}
We observe that a given erroneous program rarely uses all possible error
production rules. In this work, we present a novel strategy to selecting just a
small set of error production rules that can be used with an updated \emph{Error
Correcting Earley Parser} (ECE-Parser) to repair a program with parse errors. In
the remainder of this section, we give a high-level overview of our approach
(presented in \autoref{fig:overall-approach}) by describing how to:

\begin{enumerate}

  \item Use an \emph{ECE-Parser} to repair programs with parse errors
  (\S~\ref{sec:overview:ec-parsing}),

  \item Abstract erroneous programs with their \emph{partial parses} and the
  use of Probabilistic Context-Free Grammars (PCFGs)
  (\S~\ref{sec:overview:abstraction}),

  \item Train \emph{sequence classifiers} on the abstracted programs to predict
  subsets of error rules (\S~\ref{sec:overview:train}),

  \item and \emph{Predict error rules} for new erroneous programs to
  suggest fixes (\S~\ref{sec:overview:seq-classifiers}).

\end{enumerate}

\begin{figure}[t]
\begin{minipage}[c]{0.47\linewidth}
\begin{rules}
S        $\rightarrow$ Stmts end_marker
Stmts    $\rightarrow$ Stmt \n | Stmt \n Stmts
Stmt     $\rightarrow$ FuncDef | ExprStmt
          | RetStmt | PassStmt | ...
FuncDef  $\rightarrow$ def name Params : Block
Block    $\rightarrow$ \n indent Stmts dedent
RetStmt  $\rightarrow$ return | return Args
Args     $\rightarrow$ ExprStmt | ExprStmt , Args
ExprStmt $\rightarrow$ ArExpr | ...
ArExpr   $\rightarrow$ Literal
          | ArExpr BinOp Literal
Literal  $\rightarrow$ name | number | ...
\end{rules}
\caption{Simplified Python production rules.}
\label{fig:production-rules}
\end{minipage}
\hspace{0.02\linewidth}%
\begin{minipage}[c]{0.50\linewidth}
\hspace*{-0.06\linewidth}%
\input{partial-parse-example}
\end{minipage}
\end{figure}

\subsection{Error Correcting Parsing}
\label{sec:overview:ec-parsing}

\mypara{Earley Parsing and Grammars} Earley parsing is a touchstone
algorithm in the development of parsing \citep{Earley_1970}. An Earley parser is a
\emph{top-down chart parser} that uses dynamic programming, meaning that partial
parses are stored in a data structure called a \emph{chart} and the parse trees
are built top down. An (Earley) parser accepts programs that belong to a
language that is defined by a given \emph{grammar} $G$. The grammar $G$ has a
starting symbol |S| and a set of \emph{production rules}.
\autoref{fig:production-rules} presents some simplified production rules for the
Python programming language that will help parse the program in
\autoref{fig:bad-prog}. \emph{Terminal} symbols (or \emph{tokens}) are
syntactical symbols and are here presented in lowercase
letters. Uppercase letters denote \emph{non-terminal} symbols, which are
rewritten using production rules during a parse.
For example, the non-terminal |Stmt| defines all
possible Python statements, including expressions (|ExprStmt|), return statements
(|RetStmt|), \etc \autoref{fig:partial-parse-tree-1} shows the top levels of
the parse tree for the |bar| function in \autoref{fig:bad-prog} using these
productions rules.

%
% \begin{figure}[t]
% \begin{ecode}
% New_S     (*@$$\rightarrow$@*) S | S Insert
% ...
% Block     (*@$$\rightarrow$@*) E_\n E_indent Stmts E_dedent
% RetStmt   (*@$$\rightarrow$@*) ... | E_return | E_return Args
% ...
% E_return  (*@$$\rightarrow$@*) return | (*@$\epsilon$@*) | Replace | Insert return
% E_number  (*@$$\rightarrow$@*) number | (*@$\epsilon$@*) | Replace | Insert number
% E_\n      (*@$$\rightarrow$@*) \n | (*@$\epsilon$@*) | Replace | Insert \n
% ...
% Replace   (*@$$\rightarrow$@*) return | pass | \n | ... [all terminals]
% Insert    (*@$$\rightarrow$@*) Token | Insert Token
% Token     (*@$$\rightarrow$@*) return | pass | \n | ... [all terminals]
% \end{ecode}
% \caption{Error production rules for the simplified Python grammar presented in
% \autoref{fig:production-rules}}
% \label{fig:error-rules}
% \end{figure}

\mypara{Error Correcting Parsing} Related work has presented an \emph{Error
Correcting Earley Parser} (ECE-Parser) \citep{Aho_1972},
% FIXME ^^ WRW does not think this 1972 is "recent".
which extends the original algorithm's operations, with the goal to find a
minimum-edit parse for a program with parse errors. An ECE-Parser extends the
original grammar $G$ with a set of \emph{error production rules} to create a new
\emph{error grammar} $G'$. This grammar contains error rules for three major
types of errors that the ECE-Parser considers: \emph{insertion},
\emph{deletion}, and \emph{replacement} errors.

\subsubsection{ECE Parsing Rules for Python}

In this section, we present the details of adapting the Python
production rules for an ECE-Parser.

First, the ECE-Parser adds to $G'$ a new start symbol |New_S|, the helper
symbol |Replace| that is used for replacement errors and the symbols |Insert|
and |Token| that introduce insertion errors. Additionally, for each terminal |t|
in $G$ it adds the new non-terminal |E_t| that introduces errors relevant to the
|t| symbol.

Next, in addition to the existing production rules, the error grammar $G'$ has
the following error rules. The new start symbol uses the old one with the option
of an insertion error at the end:
\begin{itemize}
  \item \lstinline{New_S  $\rightarrow$ S | S Insert}
\end{itemize}
Also, for each production rule of a non-terminal |T| in $G$, another
non-terminal error rule is added that introduces the terminal symbols |E_t|, for
each original terminal |t| it has. For example, the |Stmts|, |Block| and
|RetStmt| rules are updated as:
\begin{itemize}
  \item \lstinline{Stmts    $\rightarrow$ ... | Stmt E_\n | Stmt E_\n Stmts}
  \item \lstinline{Block    $\rightarrow$ ... | E_\n E_indent Stmts E_dedent}
  \item \lstinline{RetStmt  $\rightarrow$ ... | E_return | E_return Args}
\end{itemize}
Next, for each terminal |t| in $G$, we add four error rules of the type:
\begin{itemize}
  \item \lstinline{E_t $\rightarrow$ t | $\epsilon$ | Replace | Insert t}
\end{itemize}
These four new error rules have the following usage for each terminal |t|:
\begin{enumerate}
  \item The |E_t $\rightarrow$ t| rule will match the original terminal |t| without any
  errors. This error rule is used in cases that the \emph{non-error} version of
  the rule is needed. For example, in \break
  |Block $\rightarrow$ E_\n E_indent Stmts E_dedent| it can be the case that only
  |E_dedent| is needed to match the error and |E_\n| and |E_indent| can match
  their respective symbols.
  \item Using |E_t $\rightarrow$ $\epsilon$| a \emph{deletion} error is considered. The
  error rule will match \emph{nothing} or the \emph{empty token} $\epsilon$ in
  the program, meaning the terminal is missing.
  \item Using |E_t $\rightarrow$ Replace| a \emph{replacement} error is considered.
  |Replace| will match any terminal token that is \emph{different} than |t|,
  making a replacement possible.
  \item  The rules |E_t $\rightarrow$ Insert t| will introduce a \emph{insertion} error,
  \ie |Insert| will match any \emph{sequence} of |Token|s that are not supposed
  to precede |t| in order to make the program parse.
\end{enumerate}
% When, for example, a \texttt{Return\_Stmt} is added to the ECE-Parser's chart at
% some location $i$, then the next four options are considered:
% \begin{enumerate}
%   \item There is a \texttt{\bfseries return} at location $i+1$ of the tokenized
%   program and therefore the original \emph{non-error} production rules are used
%   to and are added to the chart at location $i+1$.
%   \item An \emph{insertion} error is considered, \ie there is a token at program
%   location $i$ that doesn't match any of the production rules. Then
%   \texttt{Err\_Return $\rightarrow$ H return} is used to introduce one or more of these
%   insertion errors together with the rules \texttt{H $\rightarrow$ H \textbar\ H Insert}
%   and \texttt{Insert $\rightarrow$ \textbf{pass} \textbar\ \textbf{raise} \textbar\
%   \textbf{return} \textbar\ ...}. When \texttt{Insert} will match an
%   extraneous token $t$ in the program, then $t$ will be ignored in the final
%   parse repair.
%   \item A \emph{deletion} error is considered when an incomplete non-terminal
%   production rule is in the chart at location $i$ and the next symbol is a
%   terminal in the rule but is not present in the token sequence. Then
%   \texttt{Err\_Return $\rightarrow$ e} is used to match an \emph{empty} token. At the final
%   stage, the relevant terminal (here a \texttt{\textbf{return}}) will be added
%   at location $i+1$ to generate a repaired program.
%   \item A \emph{replacement} error is considered here when an incomplete
%   non-terminal production rule is in the chart at location $i$ and the next
%   symbol is a terminal in the rule but is not the same in the token sequence.
%   Then \texttt{Err\_Return $\rightarrow$ Err\_Tag} is used to match the program token $t$.
%   At the final stage, the relevant terminal (here a \texttt{\textbf{return}})
%   will replace the program token $t$.
% \end{enumerate}

For example, for the terminal tokens |return|, |number| and |\n| (a new line)
the relevant error production rules are:
\begin{itemize}
  \item \lstinline{E_return  $\rightarrow$ return | $\epsilon$ | Replace | Insert return}
  \item \lstinline{E_number  $\rightarrow$ number | $\epsilon$ | Replace | Insert number}
  \item \lstinline{E_\n      $\rightarrow$ \n | $\epsilon$ | Replace | Insert \n}
\end{itemize}
Finally, the |Replace| non-terminal can match any possible terminal in $G$ to
introduce replacement errors, the |Insert| non-terminal will introduce a
sequence of insertion errors by using |Token| which also matches every terminal
and we just differentiate the name in order be able to distinguish the different
types of errors.
\begin{itemize}
  \item \lstinline{Replace  $\rightarrow$ return | pass | \n | + | ... [all terminals]}
  \item \lstinline{Insert   $\rightarrow$ Token | Insert Token}
  \item \lstinline{Token    $\rightarrow$ return | pass | \n | + | ... [all terminals]}
\end{itemize}

\input{parse-suggestion-example.tex}



% This eliminates backtracking and prevents a combinatorial explosion. Worst
% case, it has a time complexity of $O(n^3 G^2)$ for generic context-free
% grammars, where $n$ is the number of \emph{tokens} of the input program and
% $G$ is the grammar size, \ie the number of \emph{production rules} it
% includes.

% Therefore, the new and larger error correcting grammar $G'$ is introduced that
% is at least 3 times larger than $G$, making ECE-Parser not scalable for large
% programs or programs with a lot of parse errors.

\subsubsection{ECE Parsing Considerations for Python}
In this section we describe considerations that arise when using ECE-Parser for
Python directly. \autoref{fig:partial-parse-tree-2} presents a \emph{partial
parse} of the problematic statements |Stmts|$^1$ of
\autoref{fig:partial-parse-tree-1}. Considering a deletion error, the
%
|E_number $\rightarrow$ $\epsilon$| error rule is used to match the empty symbol
and generate a parse that suggests that a \emph{number} is missing after the |+|
operator. On the other hand, the
%
|E_\n $\rightarrow$ Insert \n| error rule can be used to consider a insertion
error before the new line character, basically \emph{deleting} the |+| operator.
In this case, |ArExpr $\rightarrow$ Literal| is used to parse the program
instead of |ArExpr $\rightarrow$ ArExpr BinOp Literal|.

The ECE-Parser is an effective approach on finding minimum distance parses for
programs than do not belong in the given programming language, \ie have parse
errors. However, this parsing algorithm has limited use in large real-world
programming languages, \eg Python or Java, and more time- and memory-efficient
parsing algorithms are often used, \eg LR parsing \etc \citep{Knuth_1965,
Chapman_1987}. For example, Python has \emph{$91$ terminal symbols} (including
the program's |end_marker|) which means that for all the cases of the error
rules |E_t| (excluding the non-error base case |E_t $\rightarrow$ t|), |Replace|
and |Token|, \emph{$455$ terminal error rules} have to be added to the grammar
$G'$. The Python grammar that we used has also \emph{$283$ production rules},
from which \emph{$182$ rules} have terminals in them, meaning another
\emph{$182$ error rules} need to be added. Accounting the four helper production
rules, \eg for the new start symbol, the new grammar $G'$ has \emph{$641$ new
error production rules}. This large amount of error rules renders the ECE-Parser
not scalable for large programs or programs with a lot of parse errors when
using real-world programming languages.

One of our insights,
as seen in our running example in \autoref{fig:two-partial-parses}, is that only
a handful of error rules are relevant to each parse error. Therefore, we
propose to improve ECE-Parsing's scalability by only adding \emph{a
small set} of error production rules, \ie keeping the size of $G'$ limited and
only slightly larger than the original grammar $G$. We propose to do so by
\emph{training classifiers} to select a small set of error rules only relevant
to the parse error. However, the program token sequences that we can use
may have irrelevant information, \eg the |foo| function in our example in
\autoref{fig:example-prog} that does not contribute to the parse error. To
address this problem, we propose to further \emph{abstract} our program token
sequences.

\subsection{Abstracting program token sequences}
\label{sec:overview:abstraction}

We need to work on programs that have parse errors, which means that we can not
generate a parse tree for them to perform the usual of analyses associated with
automated program repair \citep{Sakkas_2020, Martinez_2013, Gulwani_2018,
Wang_2018} or program analysis in general.
%
The highest form of abstraction that we can use is the output of the
\emph{lexer}. The lexer tokenizes the program and returns a program \emph{token
sequence} with minimal abstractions, \eg no variable names, defining the
underlying statement blocks \etc, as shown in \autoref{fig:prog-seq}. This token
sequence is appropriate for training our sequence classifier models, but it can
be large or contain irrelevant context for repairing the parse error at hand.

\mypara{Solution: Abstract with Partial Parses}
A parser will hold some internal state of the partial parses it has
generated until a parse error is encountered. The parser will then fail and
return an error pointing to the location that lead to that parse error. However,
these partial parses provide useful information and abstraction for the program
at hand. In our example, the partial parse tree shown in
\autoref{fig:partial-parse-tree-1} can be generated by an Earley parser. Using
this partial parse tree we can extract the \emph{abstracted token sequence}
shown in the bottom of \autoref{fig:abstract-prog-seq}.

Any \emph{completed} production rules can be used to replace the relevant
\emph{token sub-sequences} with the high-level \emph{non-terminal}. For example,
the function |foo| is completely parsed, since it had no parse errors and the
highest level rule that can be used to abstract it is
%
|Stmt $\rightarrow$ FuncDef|. On another example in
\autoref{fig:partial-parse-tree-1}, we observed that
%
|Params $\rightarrow$ ( name )| is another completed production rule, therefore
the parameters in the |bar| function can be replaced with |Params|. The
production rule for |FuncDef|, however, is \emph{incomplete} since the last
statement |Stmt| (under |Stmts|$^1$) has a parse error as shown in
\autoref{fig:partial-parse-tree-2}.

The generation of this abstraction, however, poses another difficulty. Earley
parsing collects a large amount of partial parses (via dynamic programming)
until the program is fully parsed. That means at each program location, multiple
partial parses can be chosen to abstract our programs. This \emph{ambiguity} can
be seen even in the two suggested repairs in \autoref{fig:two-partial-parses}:
if we delete the colored nodes in \autoref{fig:adding-partial} and
\autoref{fig:deleting-partial} we obtain two possible partial parses for our
program, the first one matching \autoref{fig:partial-parse-tree-2} and the
second one not shown here. To chose between partial parses to abstract the
program around the parse error, we propose the use of \emph{Probabilistic
Context-Free Grammars}.


\begin{figure}[t]
\centering
\begin{minipage}[t]{0.56\linewidth}
\centering
\begin{ecode}
def name(name): \n
indent return name + number \n
dedent \n

def name(name): \n
indent name = name(name) + number \n
return name + \n
dedent end_marker
\end{ecode}
\subcaption{The token sequence generated by the lexer for the program.}
\label{fig:prog-seq}
\end{minipage}%
\hspace{0.02\linewidth}%
\begin{minipage}[t]{0.42\linewidth}
\centering
\begin{ecode}
Stmt \n

def name Params: \n
indent Stmt \n
return Expr BinOp \n
dedent end_marker
\end{ecode}
\subcaption{The abstracted token sequence for the same program.}
\label{fig:abstract-prog-seq}
\end{minipage}
\caption{The token sequences for the Python program example in \autoref{fig:example-prog}.}
\end{figure}

\mypara{Probabilistic Context-Free Grammars}
Even if the original grammar $G$ is not ambiguous, generating partial parses can
be ambiguous. Earley parsing is a dynamic programming technique and internally
keeps all possible parses for a program token sequence. Probabilistic
Context-Free Grammars (PCFGs) have been used in previous work
\citep{Collins_2013, Jelinek_1992} to select \emph{complete} parses for
ambiguous grammars. A PCFG associates each of its production rules with a
\emph{weight} or \emph{probability}. Each parse tree that is generated during
parsing is also associated with a probability. The probability of a parse tree
is simply the \emph{product} of the used rules' probabilities. Finally, the tree
with the highest probability is selected as a final parse tree.

A PCFG can be learned \citep{Collins_2013} by counting the production
rules used to parse a number of programs belonging to that language.
\autoref{fig:weighted-production-rules} has the learned probabilities for the
example Python grammar. We observe, for example, that |ReturnStmt| has two
possible production rules and almost $98.4\%$ of the times a |return| is
followed by an argument list. Additionally, $62.6\%$ of the times a |Stmt| is an
|ExprStmt| and only $7.6\%$ of the times it is a |RetStmt|.

% Even if the original grammar $G$ is not ambiguous, generating partial parses
% can be ambiguous as we've shown. Earley parsing is an dynamic programming
% technique and internally keeps all possible parses from the begging of the
% token sequence until a position $i$ in its charts. Therefore, when there is a
% parse error at token $i$, it is possible to have more than one partial parses
% to choose from in order to generate the abstracted token sequence.

% For the running example, if we focus on the \texttt{return} statement of the
% program, there are two possible partial parses in the parser chart, as shown in
% \autoref{fig:two-partial-parses} for example, that we can use to repair the
% program. \autoref{fig:adding-partial} would lead to a solution that completes
% the returned arithmetic expression by adding the blue node, which is another
% arithmetic expression and possibly just another number, while the
% \autoref{fig:deleting-partial} would produce a solution that returns just the
% number by deleting the arithmetic operator in the red node.

We apply this approach to select a \emph{partial parse} from the Earley parser,
instead of complete parses. The selected partial parse can then be used to
generate an abstracted token sequence, as described above. In our example, the
probability that would be assigned to the partial parse for |Stmts|$^1$ in
\autoref{fig:adding-partial} (only the sub-tree without the colored error nodes)
is the product of the probabilities of the production rules |Stmts|
$\rightarrow$ |Stmt \n|, |Stmt| $\rightarrow$ |RetStmt|, |RetStmt| $\rightarrow$
|return Args|, |Args| $\rightarrow$ |ExprStmt| \etc which is $38.77\%\ \cdot\
7.59\%\ \cdot\ 98.39\%\ \cdot\ 99.20\%\ \cdot\ \dots =
4.57\text{\textperthousand}$, while the partial parse for |Stmts|$^1$ in
\autoref{fig:deleting-partial} would similarly be calculated as
$47.61\text{\textperthousand}$, making it the proper choice for the abstraction
of the program.

\begin{figure}[t]
\begin{rules}
S        $\rightarrow$ Stmts end_marker ($p = \underline{100.0\%}$)
Stmts    $\rightarrow$ Stmt \n ($p = \underline{38.77\%}$) | Stmt \n Stmts ($p = \underline{61.23\%}$)
Stmt     $\rightarrow$ ExprStmt ($p = \underline{62.64\%}$) | RetStmt ($p = \underline{ 7.59\%}$) | ...
RetStmt  $\rightarrow$ return ($p = \underline{ 1.61\%}$) | return Args ($p = \underline{98.39\%}$)
Args     $\rightarrow$ ExprStmt ($p = \underline{99.20\%}$) | ...
ExprStmt $\rightarrow$ ArExpr ($p = \underline{29.40\%}$) | ...
ArExpr   $\rightarrow$ Literal ($p = \underline{86.89\%}$) | ArExpr BinOp Literal ($p = \underline{13.11\%}$)
Literal  $\rightarrow$ name ($p = \underline{64.89\%}$) | number ($p = \underline{20.17\%}$) | ...
\end{rules}
\caption{The production rules shown in \autoref{fig:production-rules} with
their learned \underline{probabilities}.}
\label{fig:weighted-production-rules}
\end{figure}
% S        (*@$\rightarrow$@*) Stmts end_marker        , $p = 100.0\%$
% Stmts    (*@$\rightarrow$@*) Stmt \n                 , $p = 38.77\%$
% Stmt     (*@$\rightarrow$@*) ExprStmt                , $p = 62.64\%$
% Stmt     (*@$\rightarrow$@*) RetStmt                 , $p =  7.59\%$
% RetStmt  (*@$\rightarrow$@*) return                  , $p =  1.61\%$
% RetStmt  (*@$\rightarrow$@*) return Args             , $p = 98.39\%$
% Args     (*@$\rightarrow$@*) ExprStmt                , $p = 99.20\%$
% ExprStmt (*@$\rightarrow$@*) ArExpr                  , $p = 29.40\%$
% ArExpr   (*@$\rightarrow$@*) Literal                 , $p = 86.89\%$
% ArExpr   (*@$\rightarrow$@*) ArExpr BinOp Literal    , $p = 13.11\%$
% Literal  (*@$\rightarrow$@*) name                    , $p = 64.89\%$
% Literal  (*@$\rightarrow$@*) number                  , $p = 20.17\%$
% Stmts    (*@$\rightarrow$@*) Stmt \n Stmts           , $p = 61.23\%$
% Stmt     (*@$\rightarrow$@*) FuncDef                 , $p =  7.86\%$
% Stmt     (*@$\rightarrow$@*) PassStmt                , $p =  0.13\%$
% FuncDef  (*@$\rightarrow$@*) def name Params : Block , $p = 99.10\%$
% Block    (*@$\rightarrow$@*) \n indent Stmts dedent  , $p = 99.29\%$
% Args     (*@$\rightarrow$@*) ExprStmt , Args         , $p =  0.80\%$

\subsection{Training Sequence Classifiers}
\label{sec:overview:train}
The abstracted token sequences we extracted from the partial parses present us
with shorter abstracted sequences that maintain much of the program's context
along with some hints of potential intended structure for the program due to the
PCFG usage. Additionally, recent machine learning and
natural language processing (NLP) \citep{Sutskever_2014, Hardalov_2018}
work has used \emph{sequence models} to learn underlying patterns in their data.
These models stand out in domains where the input is a sequence and where feature
extraction methods are not readily available~\citep{Sutskever_2014}.
% to utilize in order to use the older classic neural network models

% FIXME: Maybe delete this sequnce models
\mypara{Sequence Models}
Sequence-to-sequence (seq2seq) architectures transform an input sequence
of tokens into a new sequence \citep{Sutskever_2014}. These architectures
consist of an \emph{encoder} and a \emph{decoder}. The encoder transforms the
input token sequence into a \emph{$n$-dimensional abstract vector} that captures
all the essence and context of the input sequence. This vector does not
necessarily have a physical meaning and is instead an internal representation of
the input sequence into a higher dimensional space. The abstract vector is given
as an input to the decoder, which in turn transforms it into a output sequence.
Both the encoder and the decoder can be an (older) Long-Short-Term-Memory
(LSTM)-based model \citep{Hochreiter_1997} or a more modern and state-of-the-art
\emph{Transformer} \citep{Vaswani_2017}.

\mypara{Transformer Classifier}
We desire to train a model that can correctly predict a small set of error
production rules for a given abstracted token sequence. We approach this
problem using \emph{supervised multi-class classification}. A \emph{supervised}
learning problem is one where, given a labeled training set, the task is to
learn a function that accurately maps the inputs to output labels and
generalizes to future inputs. In a \emph{classification} problem, the function
we are trying to learn maps inputs to a discrete set of two or more output
labels, called \emph{classes}. We thus encode the task of learning a
function that will map token sequences of erroneous programs to a small set of
error production rules as a \emph{multi-class} classification (MCC) problem. We
use an \emph{encoder transformer} to encode the input sequences into abstract
vectors that we can then be fed into a \emph{\dnn} classifier to train and make
predictions. Exploiting recent advances in machine learning, we use deep and
dense architectures \citep{Schmidhuber_2015} for more accurate predictions.

\mypara{Training From a Dataset}
Given a dataset of fixed parse errors, such as \autoref{fig:example-prog}, we
extract the small set of error rules needed for each program to make it parse
with an ECE-Parser. Running the ECE-Parser on every program in the dataset with
the \emph{full set} of error production rules is prohibitively slow. Therefore,
we extract the erroneous and fixed program token-level differences or
\emph{token diffs} and map them to \emph{terminal error production rules}. The
non-terminal error rules can be inferred using the grammar and the terminal
rules. After this procedure, we run ECE-Parser with the extracted error rules to
confirm which ones would make the program parse and assign them as
\emph{labels}. For example, the diff for the program pair in
\autoref{fig:example-prog} would show the deleted |+| operator, thus extracting
the error rules |Token $\rightarrow$ +| and |E_\n $\rightarrow$ Insert \n|,
since the extra token |+| precedes a newline character |\n|. Similarly, if a
token |t| is added in the fixed program, the error rule
%
|E_t $\rightarrow$ $\epsilon$| is added and if a token |t| replaces a token |a|
in the fix the error rules |E_t $\rightarrow$ Replace| and
%
|Replace $\rightarrow$ a| are added.

\subsection{Predicting Error Rules with Sequence Classifiers}
\label{sec:overview:seq-classifiers}
The learned Transformer classifier model, which has be trained on the updated
error-rule-labeled data set can now be used to make predictions for new
erroneous programs. Additionally, neural networks have the advantage of
associating each class with a \emph{confidence score} that can be interpreted as
the model's confidence of each class being correct for a given input. Therefore,
confidence scores can be used to rank error rule predictions for new programs
and select the top-$N$ ones that will maintain a decent performance when used
with the ECE-Parser.

For our running example, in \autoref{fig:bad-prog}, we abstract the program
shown in \autoref{fig:abstract-prog-seq} and then we predict the error
production rules for it with the trained Transformer classifier. We rank the
full set of \emph{terminal} error rules based on their predicted error score
from the classifier and return the \emph{top 10} predictions for our example.
Therefore, the predicted set of error rules is the following:
%
|E_number $\rightarrow$ $\epsilon$|, |E_number $\rightarrow$ Insert number|,
%
|E_\n $\rightarrow$ Insert \n|, |E_( $\rightarrow$ $\epsilon$|, \linebreak
%
|E_return $\rightarrow$ $\epsilon$|, |Token $\rightarrow$ )|,
%
|Token $\rightarrow$ +|, |Token $\rightarrow$ :|, |Token $\rightarrow$ name|,
%
|Token $\rightarrow$ number|.

The classifier predicts mostly relevant error rules such as the ones that use
|E_number|, |E_\n| and |E_return| for example, as we showed previously. There
are also rules that are not very relevant to this parse error but the classifier
predicts probably due to them being common parse errors, \eg
%
|Token $\rightarrow$ )|, |Token $\rightarrow$ :|. Finally, we added the
\emph{non-terminal} error rules needed to introduce these errors, which can be
inferred by them. For example, we can infer
%
|Stmts $\rightarrow$ Stmt E_\n|, |Stmts $\rightarrow$ Stmt E_\n Stmts| and
%
\lstinline{Block $\rightarrow$ E_\n indent Stmts dedent} from |E_\n| (we don't
need |E_indent| or |E_dedent| here since no such terminal error rules were
predicted).

We then parse the program in \autoref{fig:bad-prog} with the ECE-Parser and
these specific error rules to generate a valid parse. We observe that it takes
our implementation (as we will show later in depth) around \emph{2 seconds} to
generate a valid parse, which is also the one that leads to the actual user
repair in \autoref{fig:fixed-prog}! On the other hand, when we use a baseline
ECE-Parser with the full set of error rules it takes \emph{2 minutes and 55
seconds} to generate a valid parse, which is, additionally, not the expected
user parse, but the one presented in \autoref{fig:adding-partial}. These
examples demonstrate the effectiveness of accurately \emph{predicting error
rules using sequence classifiers, which are trained on abstracted token
sequences}.

In the next three sections, we describe in depth the specifics of our approach
by defining all the methods in \autoref{fig:api}. We start by presenting the
program abstraction (\autoref{sec:prog-abstract}) using partial parses and a
learnt PCFG, we then explain how we train sequence models for making error rule
predictions (\autoref{sec:seq-classifiers}) and, finally, we demonstrate our
algorithms for building \toolname, an approach for efficiency parsing erroneous
programs.
