\section{Overview}
\label{sec:overview}

We begin with an overview of our approach to suggesting fixes for programs with
parse errors by fine-tuning error correcting parsers with prior knowledge of the
processes (novice) programmers follow to repair their errors.

\begin{figure}[h]
\centering
\begin{minipage}[c]{0.48\linewidth}
\begin{ecode}
def foo(a):
  return a + 42

def bar(a):
  b = foo(a)
  return b +
\end{ecode}
\subcaption{A Python program with two functions that manipulate an integer,
where the second one has a parse error.}
\label{fig:bad-prog}
\end{minipage}%
\hspace{0.02\linewidth}%
\begin{minipage}[c]{0.48\linewidth}
\begin{ecode}
def foo(a):
  return a + 42

def bar(a):
  b = foo(a)
  return b + 17
\end{ecode}
\subcaption{A possible fixed version for the previous example that has no parse
 errors.}
\label{fig:fixed-prog}
\end{minipage}
\caption{A simple Python program example}
\end{figure}


\mypara{The Problem} Consider the simple program shown in
\autoref{fig:bad-prog}. The program defines two functions \texttt{foo} and
\texttt{bar}. Function \texttt{foo} adds some number to the argument variable
\texttt{a} and returns it. Function \texttt{bar} calls \texttt{foo} with the
argument variable \texttt{a} and returns the result. However, there is an extra
\texttt{+} operator after the \texttt{return} statement. The extra \texttt{+}
can either be deleted or the programmer most likely intended to add another
number like the fix shown in the bottom of \autoref{fig:fixed-prog}. This is a
\emph{syntax or parse error} and such parse errors can easily go unnoticed
\citep{Denny_2012, Ahadi_2018, VanDerSpek_2005} by programmers when they are
part of bigger programs. Our goal is to use historical data of how programmers
have fixed similar errors in their programs to automatically and rapidly guide
programmers to come up with candidate solutions like the one above.


\mypara{Solution: Error Correcting Parsers}
One approach to make such erroneous programs parse is to use \emph{Error
Correcting Parsers} (EC-Parsers) \citep{Aho_1972}. EC-Parsers are based on
\emph{dynamic programming algorithms} and have been used in recent literature to
automatically infer the intended parse tree for a program that doesn't belong to
a certain programming language, \ie a program that has a syntax error. These
dynamic programming parsers have been used along side with \emph{error
correcting production rules} \citep{Aho_1972} to generate a valid parse by
making minimal edits to the original program.

\mypara{Another Problem}
However, these error-correcting parsers have a time complexity that is cubed on
the input program size and squared on the grammar size used. Futhermore, there
is the added cost of the large number of additional production rules these
parsers have to consider while having an explosion of the internal states by
maintaining the cost of each possible parse. Therefore, they are not tractable
or scalable options for real-world programming languages and programs.


\mypara{Solution: Train classifier to predict error rules}
The original Earley parsing is a fairly efficient parsing algorithm
\citep{Earley_1970} based on dynamic programming. Its error-correcting version
becomes very inefficient though for larger real-world grammars, since it
requires the addition of a large number of error production rules as mentioned
before. However, not every erroneous program needs all of those rules to be
parsed. In this work, we present a novel strategy to selecting just a small set
of error production rules that can be used with an updated \emph{Error
Correcting Earley Parser} (ECE-Parser) to repair a program with parse errors.
%
To enable the efficient usage of a scalable ECE-Parser we decompose the problem into 3
steps:
%
First, \emph{learn} the set of error productions rules each program with a parse
error needs from a corpus of fixed programs.
%
Second, \emph{predict}, for each erroneous program, a small set of error rules.
%
Third, \emph{parse} an erroneous program with the predicted rules and generate a
fix.
%
In the remainder of this section, we give a high-level overview
of our approach by describing how to:

\begin{enumerate}

  \item Use an \emph{Error Correcting Earley Parser} (ECE-Parser) to repair programs
  with parse errors (\S~\ref{sec:overview:ec-parsing}),

  \item Abstract erroneous programs with their \emph{partial parses} by using
  Probabilistic Context-Free Grammars (PCFGs)
  (\S~\ref{sec:overview:abstraction}),

  \item Train \emph{sequence classifiers} on the abstracted programs to predict
  small sets of error production rules (\S~\ref{sec:overview:train}).

\end{enumerate}

\subsection{Error Correcting Parsing}
\label{sec:overview:ec-parsing}

\begin{figure}[t]
\begin{ecode}
S        (*@$\rightarrow$@*) Stmts end_marker
Stmts    (*@$\rightarrow$@*) Stmt \n | Stmt \n Stmts
Stmt     (*@$\rightarrow$@*) FuncDef | ExprStmt | RetStmt | PassStmt | ...
FuncDef  (*@$\rightarrow$@*) def name Params : Block
Block    (*@$\rightarrow$@*) \n indent Stmts dedent
RetStmt  (*@$\rightarrow$@*) return | return Args
Args     (*@$\rightarrow$@*) ExprStmt | ExprStmt , Args
ExprStmt (*@$\rightarrow$@*) ArExpr | ...
ArExpr   (*@$\rightarrow$@*) Literal | ArExpr BinOp Literal
Literal  (*@$\rightarrow$@*) name | number | ...
...
\end{ecode}
\caption{Simplified production rules of the Python grammar}
\label{fig:production-rules}
\end{figure}

\mypara{Earley Parsing and Grammars} Earley parsing has been a touch-stone
algorithm in the history of parsing \citep{Earley_1970}. An Earley parser is a
\emph{top-down chart parser} that uses dynamic programming, meaning that partial
parses are stored in a data structure called a \emph{chart}, which can later be
re-used, and the parse tree is built from the top down. An (Earley) parser
accepts programs that belong into a language that is defined by a given
\emph{grammar} $G$. A grammar has a starting symbol \texttt{S} and a set of
\emph{production rules}. \autoref{fig:production-rules} presents some simplified
production rules for the Python language that will help parse the program in
\autoref{fig:bad-prog}. \emph{Terminal} symbols (or \emph{tokens}) are symbols
that a Python program consists of and are presented here in lowercase letters.
Symbols that start with a capital letter here represent \emph{non-terminal}
symbols. For example, the non-terminal \texttt{Stmt} defines all the possible
Python statements, \ie expressions (\texttt{ExprStmt}), return statements
(\texttt{ReturnStmt}), \etc

\input{partial-parse-example}
%
% \begin{figure}[t]
% \begin{ecode}
% New_S     -> S | S Insert
% ...
% Block     -> E_\n E_indent Stmts E_dedent
% RetStmt   -> ... | E_return | E_return Args
% ...
% E_return  -> return | (*@$\epsilon$@*) | Replace | Insert return
% E_number  -> number | (*@$\epsilon$@*) | Replace | Insert number
% E_\n      -> \n | (*@$\epsilon$@*) | Replace | Insert \n
% ...
% Replace   -> return | pass | \n | ... [all terminals]
% Insert    -> Token | Insert Token
% Token     -> return | pass | \n | ... [all terminals]
% \end{ecode}
% \caption{Error production rules for the simplified Python grammar presented in
% \autoref{fig:production-rules}}
% \label{fig:error-rules}
% \end{figure}

\mypara{Error Correcting Parsing} Recent work has presented an \emph{Error
Correcting Earley Parser} (ECE-Parser) \citep{Aho_1972}, which extends the
original algorithm's operations, with the goal to find a minimum-edit parse for
a program with parse errors. An ECE-Parser extends the original grammar $G$ with
a set of \emph{error production rules} to create a new \emph{error grammar}
$G'$. This grammar contains error rules for three major types of errors that the
ECE-Parser considers, \ie it adds error rules that account for \emph{insertion},
\emph{deletion}, \emph{replacement} errors.

First of all, the ECE-Parser adds a new start symbol |New_S|, the helper symbol
|Replace| that is used for replacement errors and the symbols |Insert| and
|Token| that introduce insertion errors. Additionally, for each terminal |t| it
adds the new non-terminal |E_t| that introduces errors relevant to the |t|
symbol.

Next, in addition to the existing production rules, the error grammar $G'$ has
the following error rules. The new start symbols uses the old one with the
option of an insertion error at the end:
\begin{itemize}
  \item \lstinline{New_S  -> S | S Indent}
\end{itemize}
Also, for each non-terminal |T| it adds some non-terminal error rules that
introduce the terminal errors when the parsing algorithm is executed. For
example, the |Block| and |RetStmt| rules are updated as:
\begin{itemize}
  \item \lstinline{Block    -> ... | E_\n E_indent Stmts E_dedent}
  \item \lstinline{RetStmt  -> ... | E_return | E_return Args}
\end{itemize}
Next, for each terminal |t|, we add four error rules of the type:
\begin{itemize}
  \item \lstinline{E_t -> t | $\epsilon$ | Replace | Insert t}
\end{itemize}
These four new error rules has the following usage for each terminal |t|:
\begin{enumerate}
  \item The |E_t -> t| rule will match the original terminal |t| without any
  errors. This error rule is used in cases that the \emph{non-error} version of
  the rule is needed. For example, in \break
  |Block -> E_\n E_indent Stmts E_dedent| it can be the case that only
  |E_dedent| is needed to match the error and |E_\n| and |E_indent| can match
  their respective symbols.
  \item Using |E_t ->| $\epsilon$ a \emph{deletion} error is considered. The
  error rule will match \emph{nothing} or the \emph{empty token} $\epsilon$ in
  the program, meaning the terminal is missing.
  \item Using |E_t -> Replace| a \emph{replacement} error is considered.
  |Replace| will match any terminal token that is \emph{different} than |t|,
  making a replacement possible.
  \item  The rules |E_t -> Insert t| will introduce a \emph{insertion} error,
  \ie |Insert| will match any \emph{sequence} of |Token|s that are not supposed
  to precede |t| in order to make the program parse.
\end{enumerate}
% When, for example, a \texttt{Return\_Stmt} is added to the ECE-Parser's chart at
% some location $i$, then the next four options are considered:
% \begin{enumerate}
%   \item There is a \texttt{\bfseries return} at location $i+1$ of the tokenized
%   program and therefore the original \emph{non-error} production rules are used
%   to and are added to the chart at location $i+1$.
%   \item An \emph{insertion} error is considered, \ie there is a token at program
%   location $i$ that doesn't match any of the production rules. Then
%   \texttt{Err\_Return -> H return} is used to introduce one or more of these
%   insertion errors together with the rules \texttt{H -> H \textbar\ H Insert}
%   and \texttt{Insert -> \textbf{pass} \textbar\ \textbf{raise} \textbar\
%   \textbf{return} \textbar\ ...}. When \texttt{Insert} will match an
%   extraneous token $t$ in the program, then $t$ will be ignored in the final
%   parse repair.
%   \item A \emph{deletion} error is considered when an incomplete non-terminal
%   production rule is in the chart at location $i$ and the next symbol is a
%   terminal in the rule but is not present in the token sequence. Then
%   \texttt{Err\_Return -> e} is used to match an \emph{empty} token. At the final
%   stage, the relevant terminal (here a \texttt{\textbf{return}}) will be added
%   at location $i+1$ to generate a repaired program.
%   \item A \emph{replacement} error is considered here when an incomplete
%   non-terminal production rule is in the chart at location $i$ and the next
%   symbol is a terminal in the rule but is not the same in the token sequence.
%   Then \texttt{Err\_Return -> Err\_Tag} is used to match the program token $t$.
%   At the final stage, the relevant terminal (here a \texttt{\textbf{return}})
%   will replace the program token $t$.
% \end{enumerate}

For example, for the terminal tokens |return|, |number| and |\n| (a new line)
the relevant error production rules are:
\begin{itemize}
  \item \lstinline{E_return  -> return | $\epsilon$ | Replace | Insert return}
  \item \lstinline{E_number  -> number | $\epsilon$ | Replace | Insert number}
  \item \lstinline{E_\n      -> \n | $\epsilon$ | Replace | Insert \n}
\end{itemize}
Finally, the |Replace| non-terminal can match any possible terminal in $G$ to
introduce replacement errors, the |Insert| non-terminal will introduce a
sequence of insertion errors by using |Token| which also matches every terminal
and we just differentiate the name in order be able to distinguish the different
types of errors.
\begin{itemize}
  \item \lstinline{Replace  -> return | pass | \n | ... [all terminals]}
  \item \lstinline{Insert   -> Token | Insert Token}
  \item \lstinline{Token    -> return | pass | \n | ... [all terminals]}
\end{itemize}

\input{parse-suggestion-example.tex}



% This eliminates backtracking and prevents a combinatorial explosion. Worst case,
% it has a time complexity of $O(n^3 G^2)$ for generic context-free grammars,
% where $n$ is the number of \emph{tokens} of the input program and $G$ is the
% grammar size, \ie the number of \emph{production rules} it includes. However,
% this parsing algorithm has limited use in large real-world programming
% languages, \eg Python or Java, and more time- and memory-efficient parsing
% algorithms are used, \eg LR parsing \etc \citep{Knuth_1965, Chapman_1987}.

% Therefore, the new and larger error correcting grammar $G'$ is introduced that
% is at least 3 times larger than $G$, making ECE-Parser not scalable for large
% programs or programs with a lot of parse errors.

However, ECE-Parser is an effective approach on finding minimum distance parses for
programs than don't belong into the given programming language, \ie have parse
errors. A straight-forward approach to keep ECE-Parser tractable is to only add a
small set of error production rules, \ie keep the size of $G'$ limited and
slightly larger than the original grammar $G$.

\subsection{Abstracting program token sequences}
\label{sec:overview:abstraction}

We need to work on programs that have parse errors, \ie we can not generate a
parse tree for them to perform all kinds of analyses that have been used in
previous work on automated program repair \citep{Sakkas_2020,
Martinez_2013,Gulwani_2018, Wang_2018} and program analysis in general.
%
The highest form of abstraction that we can use is the output of the
\emph{lexer}. The lexer tokenizes the program and returns a program \emph{token
sequence} with minimal abstractions, \eg no variable names, defining the
underlying statement blocks \etc, as shown in \autoref{fig:fixed-prog}. This
token sequence is appropriate for training our sequence classifier models, but
as we will show later on, they can contain a lot of irrelevant context for the
parse error at hand and can lead to very large sequences for our models.

\mypara{Solution: Abstract with partial parses} A \emph{parser} will hold some
internal state of the partial parses it has generated until a parse error is
encountered. The parser will then fail and return an error pointing to the
location that lead to that parse error. However, these partial parses provide
useful information and abstraction for the program at hand. For example, the
partial parse tree shown in \autoref{fig:partial-parse-tree} can be generated by
an Earley parser for the running example program. Using this partial parse tree
we can extract the \emph{abstracted token sequence} shown in the bottom of
\autoref{fig:fixed-prog}. Any \emph{completed} production rules, \eg the
one for \texttt{Params}, can be used to replace the relevant non-terminals,
\eg the parenthesis with the function arguments in this example, with the
high-level non-terminal.

\begin{figure}[t]
\centering
\begin{minipage}[c]{0.54\linewidth}
\begin{ecode}
def name ( name ) : \n
indent return name + number \n
dedent \n

def name ( name ) : \n
indent name = name + number \n
return name + \n
dedent end_marker
\end{ecode}
\subcaption{The token sequence generated by the lexer for the program.}
\label{fig:prog-seq}
\end{minipage}%
\hspace{0.02\linewidth}%
\begin{minipage}[c]{0.44\linewidth}
\begin{ecode}
FuncDef \n

def name Params : \n
indent Stmt \n
return Expr BinOp \n
dedent end_marker
\end{ecode}
\subcaption{The abstracted token sequence for the same program.}
\label{fig:abstract-prog-seq}
\end{minipage}
\caption{A simple Python program example}
\end{figure}

\mypara{Probabilistic Context-Free Grammars} Even if the original grammar $G$ is
not ambiguous, generating partial parses can be ambiguous. Earley parsing is an
dynamic programming technique and internally keeps all possible parses from the
begging of the token sequence until a position $i$ in its charts. Therefore,
when there is a parse error at token $i$, it is possible to have more than one
partial parses to choose from in order to generate the abstracted token
sequence. For the running example, if we focus on the \texttt{return} statement
of the program, there are two possible partial parses in the parser chart, as
shown in \autoref{fig:two-partial-parses} for example, that we can use to repair
the program. \autoref{fig:adding-partial} would lead to a solution that
completes the returned arithmetic expression by adding the blue node, which is
another arithmetic expression and possibly just another number, while the
\autoref{fig:deleting-partial} would produce a solution that returns just the
number by deleting the arithmetic operator in the red node.

In that aid, Probabilistic Context-Free Grammars (PCFGs) have been used in
previous work \citep{Collins_2013, Jelinek_1992} to select \emph{complete}
parses for ambiguous grammars. A PCFG associates each of its production rules
with a \emph{weight} or \emph{probability}. Each parse tree that is generated
during parsing is also associated with a probability. Finally, the one with the
highest probability is selected as a final parse tree. A PCFG can simply be
learned \citep{Collins_2013} by counting the production rules used to parse a
number of programs belonging to that language for each terminal token.

We apply this approach to select a partial parse from the Earley parser. The
probabilities can again be learned from the \emph{fixed} programs of a large
corpus of erroneous and fixed programs. The selected partial parse can then be
used to generate an abstracted token sequence.


\subsection{Training Sequence Classifiers}
\label{sec:overview:train}
The abstracted token sequences we extracted from the partial parses present us
with shorter abstracted sequences that maintain a lot of the program's context
along with some hints of the user's intended behavior for the program due to the
PCFG usage. Additionally, recent work on machine learning, and especially on
Natural Language Processing (NLP) \citep{Sutskever_2014, Hardalov_2018}, have
utilized \emph{sequence models} to learn underlying patterns in their data.
These models stand out in domains where the input is a sequence and feature
extraction methods are not readily available to utilize in order to use the
older classic neural network models \citep{Sutskever_2014}.

\mypara{Sequence models} Sequence-to-sequence (seq2seq) architectures aim to
transform a input sequence of tokens into a new sequence \citep{Sutskever_2014}.
These architectures consist of an \emph{encoder} and a \emph{decoder}. The
encoder transforms the input token sequence into a \emph{$n$-dimensional
abstract vector} that captures all the essence and context of the input
sequence. This vector doesn't necessarily have some physical meaning and is just
an internal representation of the input sequence into a higher dimensional
space. The abstract vector is given as an input to the decoder, which in turn
transforms it into a output sequence. Both the encoder and the decoder can be an
(older) Long-Short-Term-Memory (LSTM)-based model \citep{Hochreiter_1997} or a
state-of-the-art \emph{Transformer} \citep{Vaswani_2017}.

\mypara{Transformer Classifier} We desire to train a model that can correctly
predict a small set of error production rules for a given abstracted token
sequence. We encode this problem into a \emph{supervised multi-class
classification}. A \emph{supervised} learning problem is one where, given a
labeled training set, the task is to learn a function that accurately maps the
inputs to output labels and generalizes to future inputs. In a
\emph{classification} problem, the function we are trying to learn maps inputs
to a discrete set of two or more output labels, called \emph{classes}.
Therefore, we encode the task of learning a function that will map token
sequences of erroneous programs to a small set of error production rules as a
\emph{multi-class} classification (MCC) problem. We use an \emph{encoder
transformer} to encode the input sequences into abstract vectors that we can
then feed into a \emph{Deep Neural Network (DNN)} classifier to train it and
make predictions.

\mypara{Predicting Error Rules via MCC}
Given a dataset of fixed parse errors, we can extract the small set of error
rules needed for each program to make it parse with an ECE-Parser. Running ECE-Parser on
every program in the dataset is prohibitively slow. Therefore, we extract the
erroneous and fixed program token-level differences or \emph{token diffs} and
map them to error production rules. After this prodecure, we run ECE-Parser with the
learned error rules to confirm which ones would make the program parse and
assign them as \emph{labels}.

We then train a Transformer classifier on a new updated error-rule-labeled data
set. Futhermore, Neural networks have the advantage of associating each class
with a \emph{confidence score} that can be interpreted as the model's confidence
of each class being correct for a given input. Therefore, confidence scores can
be used to rank error rule predictions for new programs and select the top-$N$
ones that will maintain a decent performance when used with the ECE-Parser. Exploiting
recent advances in machine learning, we use deep and dense architectures
\citep{Schmidhuber_2015} for more accurate predictions.
