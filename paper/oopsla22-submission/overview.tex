\section{Overview}
\label{sec:overview}

We begin with an overview of our approach to suggesting fixes for programs with
parse errors by fine-tuning error correcting parsers with prior knowledge of the
processes (novice) programmers follow to repair their errors.

\begin{figure}[h]
\centering
\begin{minipage}[c]{0.48\linewidth}
\begin{ecode}
def foo(a):
  return a + 42

def bar(a):
  b = foo(a) + 17
  return b +
\end{ecode}
\subcaption{A Python program with two functions that manipulate an integer. The second one has a parse error.}
\label{fig:bad-prog}
\end{minipage}%
\hspace{0.02\linewidth}%
\begin{minipage}[c]{0.48\linewidth}
\begin{ecode}
def foo(a):
  return a + 42

def bar(a):
  b = foo(a) + 17
  return b
\end{ecode}
\subcaption{A possible fixed version for the previous example that has no parse
 errors.}
\label{fig:fixed-prog}
\end{minipage}
\caption{A simple Python program example}
\label{fig:example-prog}
\end{figure}


\mypara{The Problem} Consider the simple program shown in
\autoref{fig:bad-prog}. The program defines two functions |foo| and |bar|.
Function |foo| adds some number to the argument variable |a| and returns it.
Function |bar| calls |foo| with the argument variable |a|, adds another number
and returns the result. However, there is an extra |+| operator after the
|return b| statement. The extra |+| mostly likely needs to be deleted, as the
programmer intended, as shown in the fixed program in \autoref{fig:fixed-prog}.
This is a \emph{syntax (or parse) error} and such parse errors can easily go
unnoticed \citep{Denny_2012, Ahadi_2018, VanDerSpek_2005} by programmers when
they are part of bigger programs. Our goal is to use historical data of how
programmers have fixed similar errors in their programs to automatically and
rapidly guide programmers to come up with candidate solutions like the one
above.


\mypara{Solution: Error Correcting Parsers}
One approach to make such erroneous programs parse is to use \emph{Error
Correcting Parsers} (EC-Parsers) \citep{Aho_1972}. EC-Parsers are based on
\emph{dynamic programming algorithms} and have been used in recent literature to
automatically infer the intended parse tree for a program that doesn't belong to
a certain programming language, \ie a program that has a syntax error. These
dynamic programming parsers have been used along side with \emph{error
correcting production rules} \citep{Aho_1972} to generate a valid parse by
making minimal edits to the original program.

\mypara{Another Problem}
However, these error correcting parsers have a time complexity that is cubed on
the input program size and squared on the grammar size used. Futhermore, there
is the added cost of the large number of additional production rules these
parsers have to consider while having an explosion of the internal states by
maintaining the cost of each possible parse. Therefore, they are not tractable
or scalable options for real-world programming languages and programs.


\mypara{Solution: Train Classifier to Predict Error Rules}
The original Earley parsing is a fairly efficient parsing algorithm
\citep{Earley_1970} based on dynamic programming. As mentioned above, the
error-correcting version is very inefficient for larger real-world grammars,
since it requires the addition of a large number of error production rules.
However, not every erroneous program needs all of the possible error rules to be
parsed. In this work, we present a novel strategy to selecting just a small set
of error production rules that can be used with an updated \emph{Error
Correcting Earley Parser} (ECE-Parser) to repair a program with parse errors.
%
To enable the efficient usage of a scalable ECE-Parser we decompose the problem
into 3 steps:
%
First, \emph{learn} the set of error productions rules each program with a parse
error needs from a corpus of fixed programs.
%
Second, \emph{predict}, for each erroneous program, a small set of error rules.
%
Third, \emph{parse} an erroneous program with the predicted rules and generate a
fix.
%
In the remainder of this section, we give a high-level overview
of our approach by describing how to:

\begin{enumerate}

  \item Use an \emph{(ECE-Parser} to repair programs with parse errors
  (\S~\ref{sec:overview:ec-parsing}),

  \item Abstract erroneous programs with their \emph{partial parses} and the
  usage of Probabilistic Context-Free Grammars (PCFGs)
  (\S~\ref{sec:overview:abstraction}),

  \item Train \emph{sequence classifiers} on the abstracted programs to predict
  subsets of error rules (\S~\ref{sec:overview:train}).

  \item \emph{Predict error rules} for new erroneous programs in order to
  suggest fixes (\S~\ref{sec:overview:seq-classifiers}).

\end{enumerate}

\subsection{Error Correcting Parsing}
\label{sec:overview:ec-parsing}

\begin{figure}[t]
\begin{rules}
S        $\rightarrow$ Stmts end_marker
Stmts    $\rightarrow$ Stmt \n | Stmt \n Stmts
Stmt     $\rightarrow$ FuncDef | ExprStmt | RetStmt | PassStmt | ...
FuncDef  $\rightarrow$ def name Params : Block
Block    $\rightarrow$ \n indent Stmts dedent
RetStmt  $\rightarrow$ return | return Args
Args     $\rightarrow$ ExprStmt | ExprStmt , Args
ExprStmt $\rightarrow$ ArExpr | ...
ArExpr   $\rightarrow$ Literal | ArExpr BinOp Literal
Literal  $\rightarrow$ name | number | ...
\end{rules}
\caption{Simplified production rules of the Python grammar}
\label{fig:production-rules}
\end{figure}

\mypara{Earley Parsing and Grammars} Earley parsing has been a touch-stone
algorithm in the history of parsing \citep{Earley_1970}. An Earley parser is a
\emph{top-down chart parser} that uses dynamic programming, meaning that partial
parses are stored in a data structure called a \emph{chart}, which can later be
re-used, and the parse tree is built from the top down. An (Earley) parser
accepts programs that belong into a language that is defined by a given
\emph{grammar} $G$. The grammar $G$ has a starting symbol |S| and a set of
\emph{production rules}. \autoref{fig:production-rules} presents some simplified
production rules for the Python programming language that will help parse the
program in \autoref{fig:bad-prog}. \emph{Terminal} symbols (or \emph{tokens})
are symbols that a Python program consists of and are here presented in all
lowercase letters. Symbols that start with a capital letter here represent
\emph{non-terminal} symbols. For example, the non-terminal |Stmt| defines all
possible Python statements, \ie expressions (|ExprStmt|), return statements
(|ReturnStmt|), \etc \autoref{fig:partial-parse-tree-1} shows the top levels of
the parse tree for |bar| function in \autoref{fig:bad-prog} using the
aforementioned productions rules.

\input{partial-parse-example}
%
% \begin{figure}[t]
% \begin{ecode}
% New_S     (*@$$\rightarrow$@*) S | S Insert
% ...
% Block     (*@$$\rightarrow$@*) E_\n E_indent Stmts E_dedent
% RetStmt   (*@$$\rightarrow$@*) ... | E_return | E_return Args
% ...
% E_return  (*@$$\rightarrow$@*) return | (*@$\epsilon$@*) | Replace | Insert return
% E_number  (*@$$\rightarrow$@*) number | (*@$\epsilon$@*) | Replace | Insert number
% E_\n      (*@$$\rightarrow$@*) \n | (*@$\epsilon$@*) | Replace | Insert \n
% ...
% Replace   (*@$$\rightarrow$@*) return | pass | \n | ... [all terminals]
% Insert    (*@$$\rightarrow$@*) Token | Insert Token
% Token     (*@$$\rightarrow$@*) return | pass | \n | ... [all terminals]
% \end{ecode}
% \caption{Error production rules for the simplified Python grammar presented in
% \autoref{fig:production-rules}}
% \label{fig:error-rules}
% \end{figure}

\mypara{Error Correcting Parsing} Recent work has presented an \emph{Error
Correcting Earley Parser} (ECE-Parser) \citep{Aho_1972}, which extends the
original algorithm's operations, with the goal to find a minimum-edit parse for
a program with parse errors. An ECE-Parser extends the original grammar $G$ with
a set of \emph{error production rules} to create a new \emph{error grammar}
$G'$. This grammar contains error rules for three major types of errors that the
ECE-Parser considers, \ie it adds error rules that account for \emph{insertion},
\emph{deletion}, and \emph{replacement} errors.

First of all, the ECE-Parser adds to $G'$ a new start symbol |New_S|, the helper
symbol |Replace| that is used for replacement errors and the symbols |Insert|
and |Token| that introduce insertion errors. Additionally, for each terminal |t|
in $G$ it adds the new non-terminal |E_t| that introduces errors relevant to the
|t| symbol.

Next, in addition to the existing production rules, the error grammar $G'$ has
the following error rules. The new start symbol uses the old one with the option
of an insertion error at the end:
\begin{itemize}
  \item \lstinline{New_S  $\rightarrow$ S | S Indent}
\end{itemize}
Also, for each production rule of a non-terminal |T| in $G$, another
non-terminal error rule is added that introduces the terminal symbols |E_t|, for
each original terminal |t| it has. For example, the |Stmts|, |Block| and
|RetStmt| rules are updated as:
\begin{itemize}
  \item \lstinline{Stmts    $\rightarrow$ ... | Stmt E_\n | Stmt E_\n Stmts}
  \item \lstinline{Block    $\rightarrow$ ... | E_\n E_indent Stmts E_dedent}
  \item \lstinline{RetStmt  $\rightarrow$ ... | E_return | E_return Args}
\end{itemize}
Next, for each terminal |t| in $G$, we add four error rules of the type:
\begin{itemize}
  \item \lstinline{E_t $\rightarrow$ t | $\epsilon$ | Replace | Insert t}
\end{itemize}
These four new error rules have the following usage for each terminal |t|:
\begin{enumerate}
  \item The |E_t $\rightarrow$ t| rule will match the original terminal |t| without any
  errors. This error rule is used in cases that the \emph{non-error} version of
  the rule is needed. For example, in \break
  |Block $\rightarrow$ E_\n E_indent Stmts E_dedent| it can be the case that only
  |E_dedent| is needed to match the error and |E_\n| and |E_indent| can match
  their respective symbols.
  \item Using |E_t $\rightarrow$ $\epsilon$| a \emph{deletion} error is considered. The
  error rule will match \emph{nothing} or the \emph{empty token} $\epsilon$ in
  the program, meaning the terminal is missing.
  \item Using |E_t $\rightarrow$ Replace| a \emph{replacement} error is considered.
  |Replace| will match any terminal token that is \emph{different} than |t|,
  making a replacement possible.
  \item  The rules |E_t $\rightarrow$ Insert t| will introduce a \emph{insertion} error,
  \ie |Insert| will match any \emph{sequence} of |Token|s that are not supposed
  to precede |t| in order to make the program parse.
\end{enumerate}
% When, for example, a \texttt{Return\_Stmt} is added to the ECE-Parser's chart at
% some location $i$, then the next four options are considered:
% \begin{enumerate}
%   \item There is a \texttt{\bfseries return} at location $i+1$ of the tokenized
%   program and therefore the original \emph{non-error} production rules are used
%   to and are added to the chart at location $i+1$.
%   \item An \emph{insertion} error is considered, \ie there is a token at program
%   location $i$ that doesn't match any of the production rules. Then
%   \texttt{Err\_Return $\rightarrow$ H return} is used to introduce one or more of these
%   insertion errors together with the rules \texttt{H $\rightarrow$ H \textbar\ H Insert}
%   and \texttt{Insert $\rightarrow$ \textbf{pass} \textbar\ \textbf{raise} \textbar\
%   \textbf{return} \textbar\ ...}. When \texttt{Insert} will match an
%   extraneous token $t$ in the program, then $t$ will be ignored in the final
%   parse repair.
%   \item A \emph{deletion} error is considered when an incomplete non-terminal
%   production rule is in the chart at location $i$ and the next symbol is a
%   terminal in the rule but is not present in the token sequence. Then
%   \texttt{Err\_Return $\rightarrow$ e} is used to match an \emph{empty} token. At the final
%   stage, the relevant terminal (here a \texttt{\textbf{return}}) will be added
%   at location $i+1$ to generate a repaired program.
%   \item A \emph{replacement} error is considered here when an incomplete
%   non-terminal production rule is in the chart at location $i$ and the next
%   symbol is a terminal in the rule but is not the same in the token sequence.
%   Then \texttt{Err\_Return $\rightarrow$ Err\_Tag} is used to match the program token $t$.
%   At the final stage, the relevant terminal (here a \texttt{\textbf{return}})
%   will replace the program token $t$.
% \end{enumerate}

For example, for the terminal tokens |return|, |number| and |\n| (a new line)
the relevant error production rules are:
\begin{itemize}
  \item \lstinline{E_return  $\rightarrow$ return | $\epsilon$ | Replace | Insert return}
  \item \lstinline{E_number  $\rightarrow$ number | $\epsilon$ | Replace | Insert number}
  \item \lstinline{E_\n      $\rightarrow$ \n | $\epsilon$ | Replace | Insert \n}
\end{itemize}
Finally, the |Replace| non-terminal can match any possible terminal in $G$ to
introduce replacement errors, the |Insert| non-terminal will introduce a
sequence of insertion errors by using |Token| which also matches every terminal
and we just differentiate the name in order be able to distinguish the different
types of errors.
\begin{itemize}
  \item \lstinline{Replace  $\rightarrow$ return | pass | \n | + | ... [all terminals]}
  \item \lstinline{Insert   $\rightarrow$ Token | Insert Token}
  \item \lstinline{Token    $\rightarrow$ return | pass | \n | + | ... [all terminals]}
\end{itemize}

\input{parse-suggestion-example.tex}



% This eliminates backtracking and prevents a combinatorial explosion. Worst
% case, it has a time complexity of $O(n^3 G^2)$ for generic context-free
% grammars, where $n$ is the number of \emph{tokens} of the input program and
% $G$ is the grammar size, \ie the number of \emph{production rules} it
% includes.

% Therefore, the new and larger error correcting grammar $G'$ is introduced that
% is at least 3 times larger than $G$, making ECE-Parser not scalable for large
% programs or programs with a lot of parse errors.

For example, \autoref{fig:partial-parse-tree-2} presents a \emph{partial parse}
of the problematic statements |Stmts|$^1$ of \autoref{fig:partial-parse-tree-1}.
Considering a deletion error, the |E_number $\rightarrow$ $\epsilon$| error rule
is used to match the empty symbol and generate a parse that suggests that a
\emph{number} is missing after the |+| operator. On the other hand, the
%
|E_\n $\rightarrow$ Insert \n| error rule can be used to consider a insertion
error before the new line character, basically \emph{deleting} the |+| operator.
In this case, |ArExpr $\rightarrow$ Literal| is used to parse the program
instead of |ArExpr $\rightarrow$ ArExpr BinOp Literal|.

The ECE-Parser is an effective approach on finding minimum distance parses for
programs than don't belong into the given programming language, \ie have parse
errors. However, this parsing algorithm has limited use in large real-world
programming languages, \eg Python or Java, and more time- and memory-efficient
parsing algorithms are used, \eg LR parsing \etc \citep{Knuth_1965,
Chapman_1987}. For example, Python has \emph{$90$ terminal symbols} which means
that for all the cases of the error rules |E_t| (excluding the non-error base
case |E_t -> t|), |Replace| and |Token|, \emph{$450$ terminal error rules} have
to be added to the grammar $G'$. The Python grammar that we used has also
\emph{$283$ production rules}, from which \emph{$182$ rules} have terminals in
them, meaning another \emph{$182$ error rules} need to be added. Accounting the
four helper production rules, \eg for the new start symbol, the new grammar $G'$
has \emph{$636$ new error production rules}. This large amount of error rules
renders the ECE-Parser not scalable for large programs or programs with a lot of
parse errors when using real-world programming languages.

As we witnessed in our running example in \autoref{fig:two-partial-parses}, only
a handful of error rules are relevant to each parse error. Therefore, a
straight-forward approach to keep ECE-Parser tractable is to only add \emph{a
small set} of error production rules, \ie keep the size of $G'$ limited and
slightly larger than the original grammar $G$. This can be achieved by
\emph{training classifiers} to select a small set of error rules only relevant
to the parse error. However, the program token sequences that we can use have
too much irrelevant information, \eg the |foo| function in our example in
\autoref{fig:example-prog} that doesn't contribute to the parse error. The
solution to this problem, is to further \emph{abstract} our program token
sequences.

\subsection{Abstracting program token sequences}
\label{sec:overview:abstraction}

We need to work on programs that have parse errors, \ie we can not generate a
parse tree for them to perform all kinds of analyses that have been used in
previous work on automated program repair \citep{Sakkas_2020,
Martinez_2013,Gulwani_2018, Wang_2018} and program analysis in general.
%
The highest form of abstraction that we can use is the output of the
\emph{lexer}. The lexer tokenizes the program and returns a program \emph{token
sequence} with minimal abstractions, \eg no variable names, defining the
underlying statement blocks \etc, as shown in \autoref{fig:prog-seq}. This token
sequence is appropriate for training our sequence classifier models, but as we
discussed already, they can contain a lot of irrelevant context for repairing
the parse error at hand and can lead to very large sequences for our models.

\mypara{Solution: Abstract with partial parses} A \emph{parser} will hold some
internal state of the partial parses it has generated until a parse error is
encountered. The parser will then fail and return an error pointing to the
location that lead to that parse error. However, these partial parses provide
useful information and abstraction for the program at hand. In our example, the
partial parse tree shown in \autoref{fig:partial-parse-tree-1} can be generated
by an Earley parser. Using this partial parse tree we can extract the
\emph{abstracted token sequence} shown in the bottom of
\autoref{fig:abstract-prog-seq}.

Any \emph{completed} production rules can be used to replace the relevant
\emph{token sub-sequences} with the high-level \emph{non-terminal}. For example,
the function |foo| is completely parsed, since it had no parse errors and the
highest level rule that can be used to abstract it is
%
|Stmt $\rightarrow$ FuncDef|. On another example in
\autoref{fig:partial-parse-tree-1}, we observed that
%
|Params $\rightarrow$ ( name )| is another completed production rule, therefore
the parameters in the |bar| function can be replaced with |Params|. The
production rule for |FuncDef|, however, is \emph{incomplete} since the last
statement |Stmt| (under |Stmts|$^1$) has a parse error as shown in
\autoref{fig:partial-parse-tree-2}.

The generation of this abstraction, however, poses another difficulty. Earley
parsing collects a large amount of partial parses until the program is fully
parsed. That means at each program location, multiple partial parses can be
chosen to abstract our programs. This \emph{ambiguity} can be seen even in the
two suggested repairs in \autoref{fig:two-partial-parses}, where if we delete
the colored nodes in \autoref{fig:adding-partial} and
\autoref{fig:deleting-partial} we get two possible partial parses for our
program, the first one matching the one in \autoref{fig:partial-parse-tree-2}
and the second one not shown here. In order to chose between partial parses to
abstract the program around the parse error, we propose the usage of
\emph{Probabilistic Context-Free Grammars}.


\begin{figure}[t]
\centering
\begin{minipage}[c]{0.56\linewidth}
\begin{ecode}
def name(name): \n
indent return name + number \n
dedent \n

def name(name): \n
indent name = name(name) + number \n
return name + \n
dedent end_marker
\end{ecode}
\subcaption{The token sequence generated by the lexer for the program.}
\label{fig:prog-seq}
\end{minipage}%
\hspace{0.02\linewidth}%
\begin{minipage}[c]{0.42\linewidth}
\begin{ecode}
Stmt \n

def name Params: \n
indent Stmt \n
return Expr BinOp \n
dedent end_marker
\end{ecode}
\subcaption{The abstracted token sequence for the same program.}
\label{fig:abstract-prog-seq}
\end{minipage}
\caption{A simple Python program example}
\end{figure}

\mypara{Probabilistic Context-Free Grammars}
Even if the original grammar $G$ is not ambiguous, generating partial parses can
be ambiguous as we've shown. Earley parsing is an dynamic programming technique
and internally keeps all possible parses for a program token sequence. In that
aid, Probabilistic Context-Free Grammars (PCFGs) have been used in previous work
\citep{Collins_2013, Jelinek_1992} to select \emph{complete} parses for
ambiguous grammars. A PCFG associates each of its production rules with a
\emph{weight} or \emph{probability}. Each parse tree that is generated during
parsing is also associated with a probability. The probability of a parse tree
is simply the \emph{product} of the used rules' probabilities. Finally, the tree
with the highest probability is selected as a final parse tree.

A PCFG can simply be learned \citep{Collins_2013} by counting the production
rules used to parse a number of programs belonging to that language. In that
aid, a large corpus of fixed erroneous programs can be used to learn the
probabilities by parsing all the fixed programs.
\autoref{fig:weighted-production-rules} has the learned probabilities for the
example Python grammar. We observe, for example, that |ReturnStmt| has two
possible production rules and almost $98.4\%$ of the times a |return| is
followed by an argument list. Additionally, $62.6\%$ of the times a |Stmt| is an
|ExprStmt| and only $7.6\%$ of the times it is a |RetStmt|.

% Even if the original grammar $G$ is not ambiguous, generating partial parses
% can be ambiguous as we've shown. Earley parsing is an dynamic programming
% technique and internally keeps all possible parses from the begging of the
% token sequence until a position $i$ in its charts. Therefore, when there is a
% parse error at token $i$, it is possible to have more than one partial parses
% to choose from in order to generate the abstracted token sequence.

% For the running example, if we focus on the \texttt{return} statement of the
% program, there are two possible partial parses in the parser chart, as shown in
% \autoref{fig:two-partial-parses} for example, that we can use to repair the
% program. \autoref{fig:adding-partial} would lead to a solution that completes
% the returned arithmetic expression by adding the blue node, which is another
% arithmetic expression and possibly just another number, while the
% \autoref{fig:deleting-partial} would produce a solution that returns just the
% number by deleting the arithmetic operator in the red node.

We apply this approach to select a \emph{partial parse} from the Earley parser,
instead of complete parses. The selected partial parse can then be used to
generate an abstracted token sequence, as described above. In our example, the
probability that would be assigned to the partial parse for |Stmts|$^1$ in
\autoref{fig:adding-partial} (only the sub-tree without the colored error nodes)
is the product of the probabilities of the production rules |Stmts|
$\rightarrow$ |Stmt \n|, |Stmt| $\rightarrow$ |RetStmt|, |RetStmt| $\rightarrow$
|return Args|, |Args| $\rightarrow$ |ExprStmt| \etc which is $38.77\%\ \cdot\
7.59\%\ \cdot\ 98.39\%\ \cdot\ 99.20\%\ \cdot\ \dots =
4.57\text{\textperthousand}$, while the partial parse for |Stmts|$^1$ in
\autoref{fig:deleting-partial} would similarly be calculated as
$47.61\text{\textperthousand}$, making it the proper choice for the abstraction
of the program.

\begin{figure}[t]
\begin{rules}
S        $\rightarrow$ Stmts end_marker ($p = \underline{100.0\%}$)
Stmts    $\rightarrow$ Stmt \n ($p = \underline{38.77\%}$) | Stmt \n Stmts ($p = \underline{61.23\%}$)
Stmt     $\rightarrow$ ExprStmt ($p = \underline{62.64\%}$) | RetStmt ($p = \underline{ 7.59\%}$) | ...
RetStmt  $\rightarrow$ return ($p = \underline{ 1.61\%}$) | return Args ($p = \underline{98.39\%}$)
Args     $\rightarrow$ ExprStmt ($p = \underline{99.20\%}$) | ...
ExprStmt $\rightarrow$ ArExpr ($p = \underline{29.40\%}$) | ...
ArExpr   $\rightarrow$ Literal ($p = \underline{86.89\%}$) | ArExpr BinOp Literal ($p = \underline{13.11\%}$)
Literal  $\rightarrow$ name ($p = \underline{64.89\%}$) | number ($p = \underline{20.17\%}$) | ...
\end{rules}
\caption{The production rules shown in \autoref{fig:production-rules} with
their learned \underline{probabilities}.}
\label{fig:weighted-production-rules}
\end{figure}
% S        (*@$\rightarrow$@*) Stmts end_marker        , $p = 100.0\%$
% Stmts    (*@$\rightarrow$@*) Stmt \n                 , $p = 38.77\%$
% Stmt     (*@$\rightarrow$@*) ExprStmt                , $p = 62.64\%$
% Stmt     (*@$\rightarrow$@*) RetStmt                 , $p =  7.59\%$
% RetStmt  (*@$\rightarrow$@*) return                  , $p =  1.61\%$
% RetStmt  (*@$\rightarrow$@*) return Args             , $p = 98.39\%$
% Args     (*@$\rightarrow$@*) ExprStmt                , $p = 99.20\%$
% ExprStmt (*@$\rightarrow$@*) ArExpr                  , $p = 29.40\%$
% ArExpr   (*@$\rightarrow$@*) Literal                 , $p = 86.89\%$
% ArExpr   (*@$\rightarrow$@*) ArExpr BinOp Literal    , $p = 13.11\%$
% Literal  (*@$\rightarrow$@*) name                    , $p = 64.89\%$
% Literal  (*@$\rightarrow$@*) number                  , $p = 20.17\%$
% Stmts    (*@$\rightarrow$@*) Stmt \n Stmts           , $p = 61.23\%$
% Stmt     (*@$\rightarrow$@*) FuncDef                 , $p =  7.86\%$
% Stmt     (*@$\rightarrow$@*) PassStmt                , $p =  0.13\%$
% FuncDef  (*@$\rightarrow$@*) def name Params : Block , $p = 99.10\%$
% Block    (*@$\rightarrow$@*) \n indent Stmts dedent  , $p = 99.29\%$
% Args     (*@$\rightarrow$@*) ExprStmt , Args         , $p =  0.80\%$

\subsection{Training Sequence Classifiers}
\label{sec:overview:train}
The abstracted token sequences we extracted from the partial parses present us
with shorter abstracted sequences that maintain a lot of the program's context
along with some hints of the user's intended behavior for the program due to the
PCFG usage. Additionally, recent work on machine learning, and especially on
Natural Language Processing (NLP) \citep{Sutskever_2014, Hardalov_2018}, have
utilized \emph{sequence models} to learn underlying patterns in their data.
These models stand out in domains where the input is a sequence and feature
extraction methods are not readily available to utilize in order to use the
older classic neural network models \citep{Sutskever_2014}.

\mypara{Sequence models} Sequence-to-sequence (seq2seq) architectures aim to
transform a input sequence of tokens into a new sequence \citep{Sutskever_2014}.
These architectures consist of an \emph{encoder} and a \emph{decoder}. The
encoder transforms the input token sequence into a \emph{$n$-dimensional
abstract vector} that captures all the essence and context of the input
sequence. This vector doesn't necessarily have some physical meaning and is just
an internal representation of the input sequence into a higher dimensional
space. The abstract vector is given as an input to the decoder, which in turn
transforms it into a output sequence. Both the encoder and the decoder can be an
(older) Long-Short-Term-Memory (LSTM)-based model \citep{Hochreiter_1997} or a
state-of-the-art \emph{Transformer} \citep{Vaswani_2017}.

\mypara{Transformer Classifier}
We desire to train a model that can correctly predict a small set of error
production rules for a given abstracted token sequence. We encode this problem
into a \emph{supervised multi-class classification}. A \emph{supervised}
learning problem is one where, given a labeled training set, the task is to
learn a function that accurately maps the inputs to output labels and
generalizes to future inputs. In a \emph{classification} problem, the function
we are trying to learn maps inputs to a discrete set of two or more output
labels, called \emph{classes}. Therefore, we encode the task of learning a
function that will map token sequences of erroneous programs to a small set of
error production rules as a \emph{multi-class} classification (MCC) problem. We
use an \emph{encoder transformer} to encode the input sequences into abstract
vectors that we can then feed into a \emph{Deep Neural Network (DNN)} classifier
to train it and make predictions.

\mypara{Predicting Error Rules via MCC}
Given a dataset of fixed parse errors, such as \autoref{fig:example-prog}, we
extract a small set of error rules needed for each program to make it parse with
an ECE-Parser. Running ECE-Parser on every program in the dataset is
prohibitively slow. Therefore, we extract the erroneous and fixed program
token-level differences or \emph{token diffs} and map them to \emph{terminal
error production rules}. The non-terminal error rules can be inferred using the
grammar and the terminal rules. After this prodecure, we run ECE-Parser with the
extracted error rules to confirm which ones would make the program parse and
assign them as \emph{labels}. For example, the diff for the program pair in
\autoref{fig:example-prog} would show the deleted |+| operator, thus extracting
the error rules |Token $\rightarrow$ +| and |E_\n $\rightarrow$ Insert \n|,
since the extra token |+| precedes a newline character |\n|. Similarly, if a
token |t| is added in the fixed program, the error rule
%
|E_t $\rightarrow$ $\epsilon$| is added and if a token |t| replaces a token |a|
in the fix the error rules |E_t $\rightarrow$ Replace| and
%
|Replace $\rightarrow$ a| are added.

We then train a Transformer classifier on a new updated error-rule-labeled data
set. Futhermore, Neural networks have the advantage of associating each class
with a \emph{confidence score} that can be interpreted as the model's confidence
of each class being correct for a given input. Therefore, confidence scores can
be used to rank error rule predictions for new programs and select the top-$N$
ones that will maintain a decent performance when used with the ECE-Parser.
Exploiting recent advances in machine learning, we use deep and dense
architectures \citep{Schmidhuber_2015} for more accurate predictions.

\subsection{Predicting Error Rules with Sequence Classifiers}
\label{sec:overview:seq-classifiers}

