\section{Related Work}
\label{sec:related-work}

There is a vast literature on automatically repairing or patching programs:
we focus on the most closely related work on providing feedback for parse
errors.

\mypara{Error-Correcting Parsers}
%
As we have already demonstrated, error-correcting parses have been proposed for
repairing syntax errors and we have extensively described ECE-Parsers
\citep{Aho_1972}. The technique presented by \citet{Burke1987} describes another
EC-Parser, which is applicable with LR and LL parsing. It uses three phases:
first attempts to repair the parse error by symbol insertions, deletions, or
substitutions. If that fails, it tries to close one or more open code blocks and if
that fails, it removes code surrounding the erroneous symbol. Finally, it uses
\emph{deferred parsing} that may be viewed as double parsing, where one main
parser moves forward as much as possible, whereas a second parser is $k$ steps
behind, so that it can backtrack to a state $k$ steps before efficiently if a
phase fails. \citet{VanDerSpek_2005} have shown that the previous approach is not
applicable in real-world languages for some specific cases (\eg multiple
function definitions) and has suggested an improvement that works with the
\textsc{JavaCC} parser generator and a form of \emph{follow-set error recovery}.
\citet{Corchuelo2002} have suggested an error-correcting version of the popular
LR parser. Rather than focusing on error production rules, this method adds
\emph{error-repair transitions} along with the regular shift/reduce operations.
It employs a simple cost model and heuristics to limit the explosion of the
repair search space. Finally, \citet{Thompson1976} has suggested using
\emph{probabilistic parsing} to overcome the drawback of selecting the
minimal-edit repair by using a PCFG to select the most \emph{probable} repair
parse. However, these approaches are impractical and inefficient for real-world
applications, as they can only successfully parse small examples or use tiny
grammars. In contrast, \toolname relies on pre-trained sequence models to
efficiently explore the repair search space for a minimal overhead in real-time
parsing.

\mypara{Sequence Models in Software Engineering}
%
\citet{Rahmani2021} and \citet{Verbruggen2021} have suggested using pre-trained
auto-regressive transformer models, such as \textsc{GPT-3} \citep{GPT2020}, to
augment pre-existing program synthesis techniques. They use pre-trained models
to acquire semantic power over smaller subproblems that can't be solved with the
syntactic power of classic program synthesis. Similar to \toolname, their work
uses established pre-existing algorithms from the NLP and PL research areas.
However, \toolname trains its own Transformer-based model to augment an
error-correcting parsing algorithm, providing more focused prior knowledge than
a pre-trained sequence model, thus making our model highly accurate.

\mypara{Sequence Models for Parsing}
%
\textsc{SynFix} \citep{Bhatia2016} and \emph{sk\_p} \citep{Pu2016} are two
systems that use seq2seq models consisting of Long Short-Term Memory networks
(LSTMs). They mostly focus on educational programming tasks in order to learn
task-specific patterns for fixing erroneous task solutions. \textsc{SynFix} uses
a model per task and uses as an input sequence the program prefix until the
error locations that the language parser provides. \emph{sk\_p} (while it does
not solely focus on syntax errors) makes sequence predictions per program line,
by considering only the abstracted context lines (previous and next lines). The
model is applied to every program line and the predictions with the highest
probabilities are selected. \toolname manages to parse and repair a large number
of programs regardless the task they are trying to solve by encoding the full
erroneous programs with a state-of-the-art Transformer model and utilizing an
EC-Parser to parse them accordingly, thus achieving a much higher accuracy.
Additionally, it uses a real-world dataset of millions of \python programs to
learn to effectively parse programs, while \textsc{SynFix} and \emph{sk\_p} are
trained on smaller datasets of correct programs that have errors manually
introduced on training, possibly skewing the predictions away from real-world
fixes.

\textsc{DeepFix} \citep{Gupta2017} is another seq2seq approach for repairing
syntactical errors in \textsc{C} programs. It relies on stacked \emph{gated
recurrent units} (GRUs) with attention and applies some simple abstraction over
the terminal tokens. The input programs are broken into subsequences for each
line and the model gets as input all the line subsequences with their associated
line numbers. \textsc{DeepFix} only predicts single line fixes and its predictions
are applied iteratively multiple times, if multiple parse errors exist or until
the parse error is fixed. \textsc{DeepFix} struggles with the same problems as
previous work, as it solely relies on the sequence models' capability to learn
the full grammar and repair programs with minimal abstraction and prior
knowledge over the language.

\emph{Lenient parsing} \citep{Ahmed_2021} presents another sequence model
approach. It uses \emph{two seq2seq Transformer models} and trains them with a
large corpus of code. One model is trained to repair and create proper nested
blocks of code, called \textsc{BlockFix}, and the second one, called
\textsc{FragFix}, repairs and parses fragments of code (\eg program statements)
within a repaired block. \textsc{BlockFix} tokenizes input program block in a
similar manner to our abstracted token sequences, by abstracting identifiers,
constants, expressions, etc., and is trained on pairs of valid and
manually-corrupted blocks. On the other hand, \textsc{FragFix} repairs on a
program-statement level within blocks (mostly focusing on missing semicolons and
commas), by using serialized versions of ASTs and error hints manually injected
on the ASTs. While this overall approach is mostly automatic, it relies on the
manual corruption of a dataset to generate erroneous programs that may not
correlate to the errors actual developers make and solely relies on the seq2seq
models to learn the underlying language model and make repairs. In contrast,
\toolname mitigates this problem by learning how programmers fixed programs from
a large corpus and by abstracting via partial parses. Additionally, our use of
EC-Parsers and the language grammar significantly improves program repairs.

\mypara{Graph models for parsing}
%
Graph-based Grammar Fix (\textsc{GGF}) \citep{Wu2020} suggested using a
\emph{Gated Graph Neural Network} encoder for the partial parse trees that can
be acquired from a LALR parser and a \emph{GRU} encoder for the parts of the
program sequence that are not parsed. This approach aims to better summarize the
context of the program in order to train more accurate models. Its models then
predict an error location in the program sequence and a token suggestion for the
repair. This single-token repair approach is applied iteratively multiple times
until the program correctly parses. While this approach is much more accurate
than any previous work, as it repairs \emph{58\%} of the syntax errors of a
real-world dataset, it still lacks the advantages of using a parser with the
actual grammar as the final step of the repairing process that \toolname takes
benefit from and relies again on the model to learn the semantics of the
language.

\mypara{Neural Machine Translation (NMT) for Program Repair}
%
\textsc{CoCoNuT} \citep{Lutellier2020} proposed a complex architecture that uses
a new \emph{context-aware NMT model} that has two separate \emph{Convolutional
Neural Network (CNN)} encoders, one for the buggy lines and one for their
surrounding lines. It also uses \emph{ensemble learning} to train NMT models of
different hyper-parameters to capture different relations between erroneous and
correct code. This approach uses a minimal level of abstraction over the input
programs, with only a subword-level tokenization to minimize the vocabulary size
and make training tractable.
%
\textsc{CURE}~\citep{Jiang_2021} suggested a similar \emph{code-aware NMT model}
that is pre-trained using unsupervised learning on correct programs. It also
uses a programming language \textsc{GPT} \citep{GPT2020} model that learns to
predict the next token in program sequences and uses beam search to maintain a
small set of accurate repairs.

\GS{TODO: we had this part in rebuttal: "we will expand Section 8 with
qualitative comparisons to the prominent CoCoNut, as well as the Facebook
TransCoder paper (which uses tests rather than a metric like BLEU). In
particular, while being careful to indicate that it would be an
apples-to-oranges comparison, we can provide readers with additional context by
reporting some of the success rates from those approaches (such as the 509/4456
for CoCoNut on large standard software defects) and indicating which of those
successes are likely to translate to this domain and which are not. We will add
clarity to that discussion by following R1â€™s suggestion that we indicate which
algorithmic aspects of our approach represent progress over such
state-of-the-art techniques (e.g., our novelty lies in applying neural and
symbolic approaches in tandem, while error correcting parsing is already
well-established)".}